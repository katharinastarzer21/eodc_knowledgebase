{"version":"1","records":[{"hierarchy":{"lvl1":"EODC Examples Notebook Gallery"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"EODC Examples Notebook Gallery"},"content":"Welcome to the EODC Examples Notebook Gallery! This page provides curated access to interactive Jupyter Notebooks that demonstrate how to work with the various services of EODC.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"EODC Examples Notebook Gallery","lvl2":"Services Overview"},"type":"lvl2","url":"/#services-overview","position":2},{"hierarchy":{"lvl1":"EODC Examples Notebook Gallery","lvl2":"Services Overview"},"content":"EODC Website\n\nEODC Portal - Explore our data and services\n\nEODC Data Catalogue (STAC)  - Dataset overview\n\nEODC Feature Service - Access to vector dataset following the OGC API - Features standard","type":"content","url":"/#services-overview","position":3},{"hierarchy":{"lvl1":"Welcome to the EODC Knowledgebase"},"type":"lvl1","url":"/about","position":0},{"hierarchy":{"lvl1":"Welcome to the EODC Knowledgebase"},"content":"Our EODC knowledgebase is a comprehensive resource hub designed to\nprovide users with a wealth of tutorials and demos. Whether\nyou’re a beginner or an expert, our platform offers valuable insights and\nstep-by-step instructions how to access and work with our services and data.\n\nIf you feel there is a topic missing, please make your suggestion to \n\nsupport@eodc.eu.","type":"content","url":"/about","position":1},{"hierarchy":{"lvl1":"ACube - Austrian Data Cube"},"type":"lvl1","url":"/acube","position":0},{"hierarchy":{"lvl1":"ACube - Austrian Data Cube"},"content":"The Austrian Data Cube (ACube) is a service offering to consume a number of different geospatial data collections. The service offering focuses on analysis and visualisation of data by providing various APIs and client software. The first version of the ACube was tailored around the open data cube (ODC) ecosystem. Recently, a new version of the Acube service offering was released, responding to lessons learned and adopting to state of the art technologies.\n\nAcube is now built around the STAC (SpatioTemporal Asset Catalog) specifications, with a particular focus on the STAC API for data discovery. To simplify user access, the data is made publicly available via HTTPS and S3. This combination of data discovery and access APIs of the ACube lowers the barriers to geospatial data use, making it easier for users to explore, analyze, and turn data into knowledge.","type":"content","url":"/acube","position":1},{"hierarchy":{"lvl1":"EODC Network of Resources Funding"},"type":"lvl1","url":"/funding","position":0},{"hierarchy":{"lvl1":"EODC Network of Resources Funding"},"content":"Next to the openEO platform offering, EODC also offers IaaS and the Sentinel-1 sigma naught datacube on the Network of Resources (NoR).\nUsers are able to book services for pre-commercial, research and educational purposes. For non-ESA projects the sponsoring is limited to the Sentinel-1 datacube and 5,000 EUR as well one year duration request.\n\nTo apply for a NoR sponsorship please follow the guidelines below:","type":"content","url":"/funding","position":1},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"1. Navigate to the website"},"type":"lvl2","url":"/funding#id-1-navigate-to-the-website","position":2},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"1. Navigate to the website"},"content":"Go to the discover website: \n\nhttps://​nor​-discover​.org/\n\nScroll down to ‘ESA offers sponsorship to eligible entities to cover the costs of trying out the various services.’\n\nClick ESA offers sponsorship\n\n\n\nOn the Sponsoring Page scroll all the way down to the NoR PORTFOLIO Button\n\n","type":"content","url":"/funding#id-1-navigate-to-the-website","position":3},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"2. EODC Sponsoring"},"type":"lvl2","url":"/funding#id-2-eodc-sponsoring","position":4},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"2. EODC Sponsoring"},"content":"On the nor-discover page find the EODC entry and select Pricing Wizard\n\n","type":"content","url":"/funding#id-2-eodc-sponsoring","position":5},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"3. Prizing Wizard"},"type":"lvl2","url":"/funding#id-3-prizing-wizard","position":6},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"3. Prizing Wizard"},"content":"In the Prizing Wizard you select your project type\n\nSelect the packages you would like to book as well as the number of units and the subscription duration","type":"content","url":"/funding#id-3-prizing-wizard","position":7},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"4. ESA Sponsorship"},"type":"lvl2","url":"/funding#id-4-esa-sponsorship","position":8},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"4. ESA Sponsorship"},"content":"At the bottom you can ask ESA for a sponsorship\n\nIf your project is an ESA project, please select yes in the dropdown menu ‘ESA Project’\n\nIf your project is not an ESA project, you cannot book IaaS\n\nDecide if you want to co-fund the costs and move on by clicking on the button Ask ESA for Sponsorship","type":"content","url":"/funding#id-4-esa-sponsorship","position":9},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"5. Finalize your application"},"type":"lvl2","url":"/funding#id-5-finalize-your-application","position":10},{"hierarchy":{"lvl1":"EODC Network of Resources Funding","lvl2":"5. Finalize your application"},"content":"On a new pageDetails of the organization/company and the project must then be entered.\n\nWhen all required fields have been filled in export your sponsoring request and send it to \n\nNoR@esa.int.","type":"content","url":"/funding#id-5-finalize-your-application","position":11},{"hierarchy":{"lvl1":"EODC Support Center"},"type":"lvl1","url":"/support","position":0},{"hierarchy":{"lvl1":"EODC Support Center"},"content":"In order to streamline support requests and better serve you, we utilize a support ticket system. Every support request is assigned a unique ticket number which you can use to track the progress and responses online.\nFor your reference, we provide complete archives and history of all your support requests.\nA valid email address is required to submit a ticket.\nOur support page can be found \n\nhere.\n\nBest regards,\nthe EODC team","type":"content","url":"/support","position":1},{"hierarchy":{"lvl1":"Argo Workflows"},"type":"lvl1","url":"/argo-workflows","position":0},{"hierarchy":{"lvl1":"Argo Workflows"},"content":"","type":"content","url":"/argo-workflows","position":1},{"hierarchy":{"lvl1":"Argo Workflows"},"type":"lvl1","url":"/argo-workflows#argo-workflows","position":2},{"hierarchy":{"lvl1":"Argo Workflows"},"content":"This user documentation provides an overview of using the Argo Workflows deployment at EODC in order to run your earth observation processing at scale.","type":"content","url":"/argo-workflows#argo-workflows","position":3},{"hierarchy":{"lvl1":"Argo Workflows","lvl2":"What is Argo Workflows?"},"type":"lvl2","url":"/argo-workflows#what-is-argo-workflows","position":4},{"hierarchy":{"lvl1":"Argo Workflows","lvl2":"What is Argo Workflows?"},"content":"","type":"content","url":"/argo-workflows#what-is-argo-workflows","position":5},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Overview","lvl2":"What is Argo Workflows?"},"type":"lvl3","url":"/argo-workflows#overview","position":6},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Overview","lvl2":"What is Argo Workflows?"},"content":"Argo Workflows is an open-source container-native workflow engine designed to orchestrate jobs on Kubernetes. It allows users to define complex workflows as directed acyclic graphs (DAGs), facilitating the automation of multi-step tasks such as data processing and machine learning pipelines. Built to handle large-scale workloads, Argo Workflows is highly efficient for cloud-native applications and integrates seamlessly with Kubernetes environments.","type":"content","url":"/argo-workflows#overview","position":7},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Who Should Use Argo Workflows at EODC?","lvl2":"What is Argo Workflows?"},"type":"lvl3","url":"/argo-workflows#who-should-use-argo-workflows-at-eodc","position":8},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Who Should Use Argo Workflows at EODC?","lvl2":"What is Argo Workflows?"},"content":"If any of the following sounds familiar, an Argo Workflow may be right for you.\n\nContainerization: Anyone who already has a container, or would like to use an existing container to be run at scale across hundres or thousands of satellite images.\n\nNo python: Anyone who already has a code base in a language other than python, and can’t leverage the power of Dask. It may be easier to build a run time and tailor a workflow solution instead porting the code to python.\n\nRegularity: Anyone who would like to run specific processing on a daily, or timely basis, the CronWorkflow would enable you to do that. The results can be uploaded to a S3 bucket and added to a STAC collection automatically.","type":"content","url":"/argo-workflows#who-should-use-argo-workflows-at-eodc","position":9},{"hierarchy":{"lvl1":"Argo Workflows","lvl2":"Submitting an Argo Workflow"},"type":"lvl2","url":"/argo-workflows#submitting-an-argo-workflow","position":10},{"hierarchy":{"lvl1":"Argo Workflows","lvl2":"Submitting an Argo Workflow"},"content":"","type":"content","url":"/argo-workflows#submitting-an-argo-workflow","position":11},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Pre-requisite","lvl2":"Submitting an Argo Workflow"},"type":"lvl3","url":"/argo-workflows#pre-requisite","position":12},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Pre-requisite","lvl2":"Submitting an Argo Workflow"},"content":"You will need to submit a request to the \n\nEODC Support Team and request access to Argo Workflows. If you want to use argo-workflows via the eodc-sdk, you currently will need a token, make sure to request this in the support request.","type":"content","url":"/argo-workflows#pre-requisite","position":13},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Via the dashboard","lvl2":"Submitting an Argo Workflow"},"type":"lvl3","url":"/argo-workflows#via-the-dashboard","position":14},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Via the dashboard","lvl2":"Submitting an Argo Workflow"},"content":"Login to the Argo Workflows \n\nDashboard using the single sign-on option.\n\nNavigate to the workflows section of the dashboard. This is the top icon on the sidebar.\n\nClick Submit New Workflow a pop up will open. A workflow can either be loaded from a file, or submitted directly as a yaml.\n\nOnce you click create workflow, a new workflow will have appeared. Monitor for its success or failure!","type":"content","url":"/argo-workflows#via-the-dashboard","position":15},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Via the eodc-sdk","lvl2":"Submitting an Argo Workflow"},"type":"lvl3","url":"/argo-workflows#via-the-eodc-sdk","position":16},{"hierarchy":{"lvl1":"Argo Workflows","lvl3":"Via the eodc-sdk","lvl2":"Submitting an Argo Workflow"},"content":"It’s possible to use the argo workflows deployments at EODC with eodc-sdk version later than 2024.9.1.\n\nRefer to the tutorial \n\nhere.","type":"content","url":"/argo-workflows#via-the-eodc-sdk","position":17},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs"},"type":"lvl1","url":"/chiller","position":0},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs"},"content":"","type":"content","url":"/chiller","position":1},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl2":"Why do I need Chiller?"},"type":"lvl2","url":"/chiller#why-do-i-need-chiller","position":2},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl2":"Why do I need Chiller?"},"content":"EODC stores data, that is not recently used, on tape infrastructure (cold storage) which has several advantages for long term storage. This data is visible on the file system, however for large processing jobs a recall to disk storage (hot storage) can be very helpful to avoid long loading times. In order to process the desired data, a file list must be created, which is then given as input to the EODC’s own program Chiller, which moves the data from the tape storage to the disk storage, this process is also called “data staging”.","type":"content","url":"/chiller#why-do-i-need-chiller","position":3},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl2":"Filelist preparation"},"type":"lvl2","url":"/chiller#filelist-preparation","position":4},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl2":"Filelist preparation"},"content":"","type":"content","url":"/chiller#filelist-preparation","position":5},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl3":"1. Requirements","lvl2":"Filelist preparation"},"type":"lvl3","url":"/chiller#id-1-requirements","position":6},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl3":"1. Requirements","lvl2":"Filelist preparation"},"content":"EODC customers who have access to the EODC network may stage data before processing. For this you need the following:\n\nA VM on EODC infrastructure to access the “/eodc/products/” folder to curate a file list that can be uploaded to chiller\n\nA script that creates the file list (example python script for Sentinel 2 data provided below)","type":"content","url":"/chiller#id-1-requirements","position":7},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl3":"2. Use case","lvl2":"Filelist preparation"},"type":"lvl3","url":"/chiller#id-2-use-case","position":8},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl3":"2. Use case","lvl2":"Filelist preparation"},"content":"In this example use case we will stage a time series of Sentinel 2 L1C data from the Sentinel 2-A Satellite. We pick a specific UTM tile which shall be staged to disk. To create the file list, we execute the python script below through a ssh terminal connected to a VM within the EODC network.","type":"content","url":"/chiller#id-2-use-case","position":9},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl3":"3. Python example","lvl2":"Filelist preparation"},"type":"lvl3","url":"/chiller#id-3-python-example","position":10},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl3":"3. Python example","lvl2":"Filelist preparation"},"content":"The following Python script searches for all files with the given UTM tile of the Sentinel 2-A satellite and saves the found file names in a list. Afterwards the file list is saved in the home folder of the VM.import os\nimport fnmatch\n\nimport numpy as np\nimport sys\nimport datetime\nimport shutil\nprint(sys.argv)\n\n\n# Define the root directory to start the search\nroot_directory = \"/eodc/products/copernicus.eu/s2a_prd_msil1c/\"\n\n# Define the search pattern (string to search for)\nsearch_pattern = \"*\"+sys.argv[1]+\"*\"\n\n# Create an empty list to store matching file paths\nmatching_files = []\nfile_names = []\n\n# Walk through the directory tree and find matching files\ncounter = 0\nfor dirpath, dirnames, filenames in os.walk(root_directory):\n    if counter % 100 == 0:\n        print(\"Searching..\" + dirpath)\n    counter += 1\n    for filename in fnmatch.filter(filenames, search_pattern):\n        matching_files.append(os.path.join(dirpath, filename))\n        file_names.append(filename)\n\n#print (file_names)\nwith open(r'~'+sys.argv[1]+'.txt', 'w') as fp:\n    for item in matching_files:\n        fp.write(\"%s\\n\" % item)\n    print(\"Done\")\n","type":"content","url":"/chiller#id-3-python-example","position":11},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl3":"4. Running the code and writing the filelist","lvl2":"Filelist preparation"},"type":"lvl3","url":"/chiller#id-4-running-the-code-and-writing-the-filelist","position":12},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl3":"4. Running the code and writing the filelist","lvl2":"Filelist preparation"},"content":"We have selected the UTM tile T60WWV for our use case.\nTo start the script, the following expression must be called. First ‘python’, then the name of the script and then the selected tile.python create_filelist_s2a_prd_msil1c.py T60WWV\n\nNow the algorithm searches for every file with the selected UTM tile. When the search is finished, Done appears (see image below). Then the target folder specified in the script can be refreshed and a .txt file with the file list should have been created. The file list is now prepared for posting it with curl to the chiller api.","type":"content","url":"/chiller#id-4-running-the-code-and-writing-the-filelist","position":13},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl2":"Staging the data with Chiller"},"type":"lvl2","url":"/chiller#staging-the-data-with-chiller","position":14},{"hierarchy":{"lvl1":"Chiller API - preparing data for large processing jobs","lvl2":"Staging the data with Chiller"},"content":"To move the data from the tape storage to the disk storage, a job containing the generated file list is started.curl https://chiller.eodc.eu/upload --data-binary  \"@~T60WWV.txt\"\n\nAfter the command has been executed, the ID of the job is the output. The ID can be used to call up the status of the job.\n\nThe status of the job can be shown with another curl command.curl https://chiller.eodc.eu/jobs/1718707176972687523\n\nWhen \"Finished\":\"true\", then the job is done and the staging is completed. Now, for example, the NDVI can be calculated faster with all files of the time series, as all data is available on disk. To do this, a Python script must be loaded again which contains the file list.","type":"content","url":"/chiller#staging-the-data-with-chiller","position":15},{"hierarchy":{"lvl1":"How to use eomEX+ data Catalogue"},"type":"lvl1","url":"/csw","position":0},{"hierarchy":{"lvl1":"How to use eomEX+ data Catalogue"},"content":"Another possibility to access EODC data of the ESA missions Sentinel-1, Sentinel-2 and Sentinel-3 is the Catalogue Service (CSW) from Open Geospatial Consortium. This can be accessed via the following link:\n\n\nhttps://​eomex​.eodc​.eu/\n\nThe data can be filtered and selected according to temporal and spatial extent. If you are searching for specific data set, you can enter one or more keywords (e.g. Sentinel-3, OLCI). The keywords are predefined and can be displayed using the question mark on the right-hand side of the input line.\nTo define the spatial extent, four coordinates must be entered in decimal degrees. The latitude is entered in decimal degrees for North and South and the longitude is entered in decimal degrees for East and West. The desired time period can also be selected.","type":"content","url":"/csw","position":1},{"hierarchy":{"lvl1":"EODC Dask Gateway"},"type":"lvl1","url":"/dask","position":0},{"hierarchy":{"lvl1":"EODC Dask Gateway"},"content":"","type":"content","url":"/dask","position":1},{"hierarchy":{"lvl1":"EODC Dask Gateway"},"type":"lvl1","url":"/dask#eodc-dask-gateway","position":2},{"hierarchy":{"lvl1":"EODC Dask Gateway"},"content":"This user documentation provides an overview of Dask Gateway, its setup at EODC, and how to use it for managing and scaling Dask clusters for large-scale data analysis.","type":"content","url":"/dask#eodc-dask-gateway","position":3},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"What is Dask Gateway?"},"type":"lvl2","url":"/dask#what-is-dask-gateway","position":4},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"What is Dask Gateway?"},"content":"","type":"content","url":"/dask#what-is-dask-gateway","position":5},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Overview","lvl2":"What is Dask Gateway?"},"type":"lvl3","url":"/dask#overview","position":6},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Overview","lvl2":"What is Dask Gateway?"},"content":"EODC offers Dask as a service by utilising \n\nDask Gateway. User can launch a Dask cluster in a shared and managed cluster environment without requring to have direct access to any cloud infrastructure resources such as VMs or Kubernetes clusters. The objetive is to lower the entrance barrier for users to run large scale data analysis on demand and in a scaleable environment.\n\nAn generic introduction of the usage of Dask Gateway can be found on the official \n\nDask Gateway documentation. In the following we will demonstrate the use of the Dask service at EODC to further support users.","type":"content","url":"/dask#overview","position":7},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Why Choose Dask Gateway at EODC?","lvl2":"What is Dask Gateway?"},"type":"lvl3","url":"/dask#why-choose-dask-gateway-at-eodc","position":8},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Why Choose Dask Gateway at EODC?","lvl2":"What is Dask Gateway?"},"content":"Dask Gateway at EODC is specifically tailored for users who need to perform large-scale data analysis, particularly in the fields of geospatial analysis, time-series data processing, and data science. Here’s why Dask Gateway is a compelling choice:\n\nEfficient Resource Management: Dask enables the parallel processing of large datasets across multiple nodes, making it ideal for computationally intensive tasks. This is especially beneficial for users dealing with massive datasets, such as satellite imagery or climate data.\n\nFlexibility and Scalability: Unlike traditional systems where users are constrained by the limitations of a single machine, Dask allows users to scale their computations across a cluster of machines. This is particularly useful for projects that require varying levels of computational power over time.\n\nSeamless Integration with Python Ecosystem: Dask is built natively for Python, making it easy to integrate with existing Python-based workflows. This is particularly relevant for data scientists and analysts who already use Python for their projects.","type":"content","url":"/dask#why-choose-dask-gateway-at-eodc","position":9},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Who Should Use Dask Gateway at EODC?","lvl2":"What is Dask Gateway?"},"type":"lvl3","url":"/dask#who-should-use-dask-gateway-at-eodc","position":10},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Who Should Use Dask Gateway at EODC?","lvl2":"What is Dask Gateway?"},"content":"Dask Gateway at EODC is suitable for:\n\nResearchers and Scientists: Who need to process large-scale environmental or geospatial data.\n\nData Scientists: Looking to perform advanced data analysis and machine learning on large datasets.\n\nEngineers and Developers: Who require scalable computing resources for data processing tasks that exceed the capacity of their local machines.\n\nIn summary, if your work involves handling and processing large datasets and you need a scalable, efficient computing environment that minimizes the burden of infrastructure management, Dask Gateway at EODC is an optimal choice.","type":"content","url":"/dask#who-should-use-dask-gateway-at-eodc","position":11},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Steps for Authorization:","lvl2":"What is Dask Gateway?"},"type":"lvl3","url":"/dask#steps-for-authorization","position":12},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Steps for Authorization:","lvl2":"What is Dask Gateway?"},"content":"Authentication via Keycloak:\n\nWhat It Is: Keycloak is an identity and access management system that EODC uses to authenticate users. Think of it as a secure gatekeeper that ensures only authorized users can access EODC’s computing resources.\n\nHow to Get Access: If you do not already have a Keycloak account, you need to request one by emailing the \n\nEODC Support Team. In your request, include your full name, organization, and the reason you need access. Once your account is set up, you will receive your login credentials.\n\nHow It Works: After you log in with your EODC credentials, Keycloak issues a JSON Web Token (JWT). This token acts like a digital pass that you will use in your interactions with the EODC Dask Gateway server.\n\nUsing a JWT (JSON Web Token):\n\nWhat It Is: A JWT is a secure token that contains information about your identity and permissions. It is automatically generated by Keycloak after you authenticate.\n\nWhy It’s Important: The Dask Gateway server checks this token to ensure you have the necessary permissions to access and manage Dask clusters. The token is handled automatically by the system, so you don’t need to manage it manually.","type":"content","url":"/dask#steps-for-authorization","position":13},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"EODC-Specific Setup"},"type":"lvl2","url":"/dask#eodc-specific-setup","position":14},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"EODC-Specific Setup"},"content":"At EODC, we provide a robust setup that ensures your data analysis workflows are smooth, scalable, and easy to manage. Here’s what you need to know:","type":"content","url":"/dask#eodc-specific-setup","position":15},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Docker at EODC","lvl2":"EODC-Specific Setup"},"type":"lvl3","url":"/dask#docker-at-eodc","position":16},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Docker at EODC","lvl2":"EODC-Specific Setup"},"content":"What It Is: Docker at EODC is used to create portable, consistent, and isolated environments that include not just your Python setup but the entire operating system and any other tools your application might need. This is especially useful for ensuring that your data analysis pipelines run identically on different machines, whether on your local system, EODC’s servers, or in the cloud.\n\nWhen to Use It: Use Docker when your project requires more than just Python libraries—such as specific system tools, dependencies that are difficult to manage with Conda, or when your application needs to run consistently across different environments. Docker is also preferred when deploying applications in production, where reproducibility and isolation are critical.\n\nWhat You Need to Do: EODC provides a standard Docker image that includes a baseline environment suitable for most data analysis tasks. You can either use this image directly or customize it to include additional dependencies specific to your project.\n\nDownload the Docker Image:\n\nPull the pre-built Docker image from EODC’s Docker registry using the following command:docker pull registry.eodc.eu/eodc/kathi_test/my-updated-image\n\nRun the Docker Container:\n\nStart the container with the following command, which will launch a Jupyter Notebook server:docker run -p 8888:8888 registry.eodc.eu/eodc/kathi_test/my-updated-image\n\nYou’ll be provided with a URL containing a token. Open this URL in your browser to access the Jupyter Notebook interface. Please note that the URL is making use of http and not https.\n\nAccessing Dask Gateway:\n\nInside the Jupyter Notebook, you’ll find pre-configured example-notebook that demonstrate how to connect to and use the Dask Gateway Service at EODC.\n\nAll necessary libraries and configurations are already included in the Docker image, so you can focus on your data analysis without worrying about setup issues.\n\nUsing the EODC Dask Gateway in Your Notebook:\n\nAuthentication: After launching the Jupyter Notebook, authenticate and connect to the EODC Dask Gateway using the code provided in the notebook. The authentication process involves using your Keycloak credentials—your email as the username and your Keycloak password.from eodc_connect.dask import EODCDaskGateway\nfrom rich.console import Console\nfrom rich.prompt import Prompt\n\nconsole = Console()\nyour_username = Prompt.ask(prompt=\"Enter your Username\")\ngateway = EODCDaskGateway(username=your_username)\n\nThis integration allows seamless access to the Dask Gateway, ensuring you can efficiently scale your computations.\n\nConnecting and Managing Clusters:\n\nWhat You Need to Do: Once authenticated, you can create and manage a Dask cluster using the following commands:cluster = gateway.new_cluster()\nclient = cluster.get_client()\ncluster\n\nImportant: Please use the widget to add/scale the Dask workers. Per default no worker is spawned, therefore no computations can be performed by the cluster. If you want to spawn workers directly via Python adaptively, please use the method described in the \n\nadaptive scaling section.\n\nResult: This will start a new Dask cluster that you can now use for your data processing tasks.","type":"content","url":"/dask#docker-at-eodc","position":17},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"What to Do if You Encounter Issues","lvl2":"EODC-Specific Setup"},"type":"lvl3","url":"/dask#what-to-do-if-you-encounter-issues","position":18},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"What to Do if You Encounter Issues","lvl2":"EODC-Specific Setup"},"content":"Compatibility Issues: If your code runs into problems because the environment on your local machine is different from the environment on the Dask clusters, try using the EODC-provided Conda environment or Docker image. This ensures everything is set up correctly.\n\nRun the following in order to make sure all dependencies are met:!pip install bokeh==2.4.2 dask-gateway==2023.1.1 cloudpickle==2.2.1 s3fs==2023.6.0 fsspec==2023.6.0 xarray==2023.7.0 dask==2023.8.0\n\nNeed Help?: If you’re not sure what to do, or if you run into any issues, have a look at the official website of \n\nDask Gateway documentation or contact the \n\nEODC Support Team for assistance.","type":"content","url":"/dask#what-to-do-if-you-encounter-issues","position":19},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"Cluster Profiles and Configuration at EODC"},"type":"lvl2","url":"/dask#cluster-profiles-and-configuration-at-eodc","position":20},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"Cluster Profiles and Configuration at EODC"},"content":"At EODC, Dask Gateway is configured with default resource allocations that suit the needs of most users. These settings can be customized according to specific project requirements.","type":"content","url":"/dask#cluster-profiles-and-configuration-at-eodc","position":21},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Default Cluster Configuration","lvl2":"Cluster Profiles and Configuration at EODC"},"type":"lvl3","url":"/dask#default-cluster-configuration","position":22},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Default Cluster Configuration","lvl2":"Cluster Profiles and Configuration at EODC"},"content":"Standard Resources:\n\nScheduler Cores: 2 CPU cores\n\nScheduler Memory: 4 GB RAM\n\nWorker Cores: 4 CPU cores\n\nWorker Memory: 8 GB RAM\n\nThese settings are the default configuration for most users, providing a balanced environment for typical data processing tasks.\n\nIdle Timeout: Clusters will automatically shut down after 6 hours of inactivity. This helps to optimize resource usage and minimize costs.","type":"content","url":"/dask#default-cluster-configuration","position":23},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Exposing and Understanding Cluster Options","lvl2":"Cluster Profiles and Configuration at EODC"},"type":"lvl3","url":"/dask#exposing-and-understanding-cluster-options","position":24},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Exposing and Understanding Cluster Options","lvl2":"Cluster Profiles and Configuration at EODC"},"content":"Before customizing your Dask cluster, it’s important to understand the available options and the limits that are in place to ensure efficient resource usage across EODC’s infrastructure.\n\nAvailable Options: EODC provides predefined cluster options that users can adjust. These include:\n\nWorker Cores (worker_cores): The number of CPU cores allocated per worker.\n\nWorker Memory (worker_memory): The amount of memory allocated per worker.\n\nDocker Image (image): The Docker image used for the cluster, which can be customized based on specific needs.\n\nLimits and Constraints:\n\nWorker Cores: Users can allocate between 2 and 8 CPU cores per worker.\n\nWorker Memory: Users can allocate between 2 GB and 16 GB of RAM per worker.\n\nThese limits are set to ensure fair usage of resources and to prevent any single user or task from monopolizing the cluster’s capacity.","type":"content","url":"/dask#exposing-and-understanding-cluster-options","position":25},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Customizing Cluster Options","lvl2":"Cluster Profiles and Configuration at EODC"},"type":"lvl3","url":"/dask#customizing-cluster-options","position":26},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Customizing Cluster Options","lvl2":"Cluster Profiles and Configuration at EODC"},"content":"Once you understand the available options and the constraints in place, you can customize your Dask cluster configuration to better align with your project’s requirements.\n\nAdjusting Resources: Based on your needs, you can modify the cluster options within the allowed limits. Here’s an example:cluster_options = gateway.cluster_options()\ncluster_options.worker_cores = 4  # Set the number of CPU cores per worker (within the range 2-8)\ncluster_options.worker_memory = \"8GB\"  # Set the memory allocation per worker (within the range 2-16 GB)\ncluster = gateway.new_cluster(cluster_options)","type":"content","url":"/dask#customizing-cluster-options","position":27},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"Managing Your Dask Cluster"},"type":"lvl2","url":"/dask#managing-your-dask-cluster","position":28},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"Managing Your Dask Cluster"},"content":"After setting up your Dask cluster, there are several important operations you can perform to manage your computational resources efficiently.","type":"content","url":"/dask#managing-your-dask-cluster","position":29},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Adaptive Scaling","lvl2":"Managing Your Dask Cluster"},"type":"lvl3","url":"/dask#adaptive-scaling","position":30},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Adaptive Scaling","lvl2":"Managing Your Dask Cluster"},"content":"Adaptive scaling allows your Dask cluster to automatically adjust the number of workers based on the current workload. This ensures that you have enough computational resources when needed, while minimizing costs when the cluster is idle.\n\nEnabling Adaptive Scaling:\n\nTo enable adaptive scaling, use the following command:cluster.adapt(minimum=2, maximum=8)\n\nIn this example, the cluster will start with 2 workers and can scale up to a maximum of 8 workers depending on the workload. Dask will add or remove workers automatically based on the tasks being processed.\n\nWhy Use Adaptive Scaling:\n\nAdaptive scaling is useful when your workload varies significantly over time, as it ensures that your cluster is always optimally sized for the tasks at hand, avoiding over-provisioning and unnecessary costs.","type":"content","url":"/dask#adaptive-scaling","position":31},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Connecting to an Existing Cluster","lvl2":"Managing Your Dask Cluster"},"type":"lvl3","url":"/dask#connecting-to-an-existing-cluster","position":32},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Connecting to an Existing Cluster","lvl2":"Managing Your Dask Cluster"},"content":"Sometimes you might need to reconnect to a Dask cluster that is already running, either to continue working or to monitor its status.\n\nHow to Connect:\n\nIf you have a list of existing clusters, you can connect to one using its name or ID:clusters = gateway.list_clusters()\ncluster = gateway.connect(cluster_name=clusters[0].name)\nclient = cluster.get_client()\n\nThis will allow you to interact with the cluster as if it were newly created, enabling you to submit new tasks or monitor ongoing computations.","type":"content","url":"/dask#connecting-to-an-existing-cluster","position":33},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Stopping a Cluster","lvl2":"Managing Your Dask Cluster"},"type":"lvl3","url":"/dask#stopping-a-cluster","position":34},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Stopping a Cluster","lvl2":"Managing Your Dask Cluster"},"content":"When you’re done with your computations, it’s important to stop the cluster to free up resources and avoid unnecessary costs.\n\nStopping a Single Cluster:\nTo stop a cluster and release its resources, use the following command:cluster.shutdown()\n\nOr for multiple clusters:for cluster in clusters: \ncluster.shutdown()\n\nThis ensures that all resources are properly freed, and no unnecessary charges are incurred.","type":"content","url":"/dask#stopping-a-cluster","position":35},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"Display Dask Dashboard to monitor execution of computations"},"type":"lvl2","url":"/dask#display-dask-dashboard-to-monitor-execution-of-computations","position":36},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"Display Dask Dashboard to monitor execution of computations"},"content":"Dask Gateway provides a dashboard to monitor your cluster’s performance. Copy the following link into a browser of your choice. Please consider the dashboard URL provided is making use of http and not https.cluster.dashboard_link\nprint(f\"Dashboard link: {dashboard_link}\")\n\nOpen the provided URL in your web browser to view the dashboard. The dashboard provides visualizations of task progress, memory usage, CPU usage, and more, which are crucial for optimizing and troubleshooting your computations.","type":"content","url":"/dask#display-dask-dashboard-to-monitor-execution-of-computations","position":37},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Example","lvl2":"Display Dask Dashboard to monitor execution of computations"},"type":"lvl3","url":"/dask#example","position":38},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl3":"Example","lvl2":"Display Dask Dashboard to monitor execution of computations"},"content":"import s3fs\nimport xarray as xr\n\ns3fs_central = s3fs.S3FileSystem(\n    anon=True,\n    use_ssl=True,\n    client_kwargs={\"endpoint_url\": \"https://s3.central.data.destination-earth.eu\"})\n\ns3fs_lumi = s3fs.S3FileSystem(\n    anon=True,\n    use_ssl=True,\n    client_kwargs={\"endpoint_url\": \"https://s3.lumi.data.destination-earth.eu\"})s3fs_central.ls(\"increment1-testdata\")\n\nRead data stored in S3 bucket at central site (Poland). The data we want to read is a single Zarr data store representing GFM flood data over Pakistan for 2022-08-30flood_map = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/2022-08-30.zarr\", s3=s3fs_central, check=False),\n                         decode_coords=\"all\",)[\"flood\"].assign_attrs(location=\"central\", resolution=20)\nflood_map\n\nRun simple computation and compute the flooded area:flooded_area_ = flood_map.sum()*20*20/1000.\nflooded_area_\n\nSo far we haven’t computed anything, so lets do the computation now on the Dask cluster.flooded_area = client.compute(flooded_area_, sync=True)\nconsole.print(f\"Flooded area: {flooded_area.data}km2\")\n\nRead data stored in S3 bucket at LUMI bridge (Finland). Data we want to read is a datacube generated from ERA-5 representing predicted rainfall data.rainfall = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/predicted_rainfall.zarr\",\n                                         s3=s3fs_lumi,\n                                         check=False),\n                        decode_coords=\"all\",)[\"tp\"].assign_attrs(location=\"lumi\", resolution=20)\nrainfallfrom datetime import datetime\nfrom attr import dataclass\n\ndef accum_rain_predictions(rain_data, startdate, enddate, extent):\n    rain_ = rain_data.sel(time=slice(startdate, enddate),\n                          latitude=slice(extent.max_y, extent.min_y),\n                          longitude=slice(extent.min_x, extent.max_x))\n    return rain_.cumsum(dim=\"time\", keep_attrs=True)*1000\n\n@dataclass\nclass Extent:\n    min_x: float\n    min_y: float\n    max_x: float\n    max_y: float\n    crs: str\n\n# compute accumulated rainfall over Pakistan\nroi_extent = Extent(65, 21, 71, 31, crs='EPSG:4326')\nacc_rain_ = accum_rain_predictions(rainfall, startdate=datetime(2022, 8, 18),\n                                             enddate=datetime(2022, 8, 30),\n                                             extent=roi_extent)\n\n# compute average rainfall for August 2022\nrain_ = rainfall.sel(time=slice(datetime(2022, 8, 1), datetime(2022, 8, 30))).mean(dim=\"time\", keep_attrs=True)*1000\nrain_\n\nAnd again run the computation on our EODC Dask cluster. First we compute the accumulated rainfall over Pakistan. Secondly we compute the average rainfall for August 2022 (monthly mean) at global scale.acc_rain = client.compute(acc_rain_, sync=True)\nacc_rain\nmean_rain = client.compute(rain_, sync=True)\nmean_rain\n\nPlot a histogram of the accumlated rainfall computed for Pakistan.acc_rain.plot()cluster.close(shutdown=True)","type":"content","url":"/dask#example","position":39},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"Additional Considerations"},"type":"lvl2","url":"/dask#additional-considerations","position":40},{"hierarchy":{"lvl1":"EODC Dask Gateway","lvl2":"Additional Considerations"},"content":"When working with Dask Gateway at EODC, it is essential to ensure that your computing environment is properly configured to handle the scale and complexity of the tasks you intend to perform. Always monitor the performance of your clusters and adjust resources accordingly to optimize both efficiency and cost. For advanced use cases, refer to the official \n\nDask Gateway documentation and consider reaching out to the \n\nEODC Support Team for further assistance.","type":"content","url":"/dask#additional-considerations","position":41},{"hierarchy":{"lvl1":"Save GFM results in cloud object store"},"type":"lvl1","url":"/gfm-dask-objectstorage","position":0},{"hierarchy":{"lvl1":"Save GFM results in cloud object store"},"content":"In a final step, we will\nread parts of the result back and plot it directly on our local machine. Here, we will use our object\nstorage based on CEPH. Please see\n\n\nhere for more\ninformation.\nFurthermore, this demo is partly based on the \n\nEODC Dask\nTutorial.\n\nAs an example, we will calculate the maximum flood extent as well as the mean of\nthe GFM likelihood values for a certain time range over an area of interest in\nPakistan.\n\n","type":"content","url":"/gfm-dask-objectstorage","position":1},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Prerequisites"},"type":"lvl2","url":"/gfm-dask-objectstorage#prerequisites","position":2},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Prerequisites"},"content":"Before we start, make sure you have installed all the necessary Python libraries and\npackages with the correct versions. It is important that the cluster and client\n(your machine) have the same versions for the key Python libraries. The easiest\nway is to create a new Python environment with the package manager of your liking.\nSee the required dependencies in the \n\nEODC cluster image repository.\n\nIn order to spin up a dedicated cluster on the EODC cluster, you will need to\nrequest an EODC account. Please follow the instructions \n\nhere.\n\n","type":"content","url":"/gfm-dask-objectstorage#prerequisites","position":3},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"First some imports"},"type":"lvl2","url":"/gfm-dask-objectstorage#first-some-imports","position":4},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"First some imports"},"content":"\n\nimport s3fs\nimport pyproj\nimport rioxarray\nimport xarray as xr\nfrom datetime import datetime\nfrom shapely.geometry import box\nfrom pystac_client import Client\nfrom odc import stac as odc_stac\nimport matplotlib.pyplot as plt\n\nfrom eodc import settings\nfrom eodc.dask import EODCDaskGateway\n\nsettings.DASK_URL = \"http://dask.services.eodc.eu\"\nsettings.DASK_URL_TCP = \"tcp://dask.services.eodc.eu:80/\"\n\n","type":"content","url":"/gfm-dask-objectstorage#first-some-imports","position":5},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Initialize cluster"},"type":"lvl2","url":"/gfm-dask-objectstorage#initialize-cluster","position":6},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Initialize cluster"},"content":"Your username of your EODC account come here, usually it is your email address\nyou have used for registration.\nAfter running the next cell, a prompt will open and ask you to enter your\npassword.\n\nyour_username = \"your.email@address.com\"\ngateway = EODCDaskGateway(username=your_username)\n\nOnce authenticated, you can specify the details of your cluster.\n\nA new cluster will be created. Use the URL, which is printed after running the\nnext cell to get an overview what is currently happening on your cluster.\n\n# Define cluster options\ncluster_options = gateway.cluster_options()\n\n# Set the number of cores per worker\ncluster_options.worker_cores = 8\n\n# Set the memory per worker (in GB)\ncluster_options.worker_memory = 16\n\n# Specify the Docker image to use for the workers\ncluster_options.image = \"ghcr.io/eodcgmbh/cluster_image:2025.2.0\"\n\n# Create a new cluster with the specified options\ncluster = gateway.new_cluster(cluster_options)\n\n# Automatically scale the cluster between 1 and 10 workers based on workload\ncluster.adapt(1, 10)  \n\n# Optionally, scale the cluster to use only one worker\n# cluster.scale(1)\n\n# Get a Dask client for the cluster\nclient = cluster.get_client()\nclient.dashboard_link\n\n","type":"content","url":"/gfm-dask-objectstorage#initialize-cluster","position":7},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Search and load data"},"type":"lvl2","url":"/gfm-dask-objectstorage#search-and-load-data","position":8},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Search and load data"},"content":"Now we will define our area (AOI) and time range of interest for which we want\nto calculate the maximum flood extent as well as the mean of the likelihood.\n\n# Define the API URL\napi_url = \"https://stac.eodc.eu/api/v1\"\n\n# Define the STAC collection ID\ncollection_id = \"GFM\"\n\n# Define the area of interest (AOI) as a bounding box\naoi = box(67.398376, 26.197341, 69.027100, 27.591066)\n\n# Define the time range for the search\ntime_range = (datetime(2022, 9, 1), datetime(2022, 10, 1))\n\n# Open the STAC catalog using the specified API URL\neodc_catalog = Client.open(api_url)\n\n# Perform a search in the catalog with the specified parameters\nsearch = eodc_catalog.search(\n    max_items=1000,             # Maximum number of items to return\n    collections=collection_id,  # The collection to search within\n    intersects=aoi,             # The area of interest\n    datetime=time_range         # The time range for the search\n)\n\n# Collect the found items into an item collection\nitems = search.item_collection()\n\nprint(f\"On EODC we found {len(items)} items for the given search query\")\n\nThe data will be lazy-loaded into a xarray.Dataset object.\n\n# Extract the coordinate reference system (CRS) from the first item's properties\ncrs = pyproj.CRS.from_wkt(items[0].properties[\"proj:wkt2\"])\n\n# Set the resolution of the data\nresolution = items[0].properties['gsd']\n\n# Specify the bands to load\nbands = [\"ensemble_flood_extent\", \"ensemble_likelihood\"]\n\n# Load the data using odc-stac with the specified parameters\nxx = odc_stac.load(\n    items, \n    bbox=aoi.bounds,   # Define the bounding box for the area of interest\n    crs=crs,   # Set the coordinate reference system\n    bands=bands,   # Specify the bands to load\n    resolution=resolution,   # Set the resolution of the data\n    dtype='uint8',   # Define the data type\n    chunks={\"x\": 1000, \"y\": 1000, \"time\": -1},  # Set the chunk size for Dask\n)\n\nCreate a ZARR store object by specifying the endpoint_url, bucket_name and\ncredentials of your object storage (e.g. EODC object store, AWS S3).\n\n# Specify the endpoint_url of your object storage\nendpoint_url = '<endpoint_url>'\n\n# Specify the name of your S3 bucket\ns3_bucket = '<bucket_name>'\n\n# Specify the credentials for accessing your S3 bucket\nkey = '<key>'\nsecret = '<secret>'\n\n# Create a S3FileSystem object\ns3fs_central = s3fs.S3FileSystem(\n    key=key,\n    secret=secret,\n    client_kwargs={'endpoint_url': endpoint_url},\n)\n\n# Specify the filename of your output ZARR file\npath = f'{s3_bucket}/gfm_flood_likelihood_pakistan_202209.zarr'\n\n# Create the ZARR store object\nzarr_store = s3fs.S3Map(root=path, s3=s3fs_central, check=False)\n\n","type":"content","url":"/gfm-dask-objectstorage#search-and-load-data","position":9},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Process on the cluster"},"type":"lvl2","url":"/gfm-dask-objectstorage#process-on-the-cluster","position":10},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Process on the cluster"},"content":"For each of the data variables, we will define an own “process graph”. The\nmaximum flood extent refers to the largest area covered by flooded pixels during\nthe specified time range. For the ensemble likelihood, we will calculate the\nmean values.\n\nAs a final step, we will trigger the computation on the Dask cluster and save\ndirectly the result to the specified ZARR store on our cloud object storage.\n\nresults = {}\n\n# ensemble_flood_extent\nvar = 'ensemble_flood_extent'\nflood_extent = xx[var].where((xx[var] != 255) & (xx[var] != 0)).sum(dim=\"time\")\nresults[var] = xr.where(flood_extent > 0, 1, 0).astype(\"uint8\")\n\n# ensemble_likelihood\nvar = 'ensemble_likelihood'\nresults[var] = xx[var].where((xx[var] != 255) & (xx[var] != 0)).mean(dim=\"time\").astype(\"uint8\")\n\n# Combine the results into a new dataset\nresult_dataset = xr.Dataset(results)\n\n# Trigger computation and save the result directly to the specified ZARR store\nresult_dataset.compute(sync=True).to_zarr(store=zarr_store, mode=\"w\")\n\n","type":"content","url":"/gfm-dask-objectstorage#process-on-the-cluster","position":11},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Shutdown cluster"},"type":"lvl2","url":"/gfm-dask-objectstorage#shutdown-cluster","position":12},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Shutdown cluster"},"content":"After successful computation we can shutdown the cluster\n\ncluster.close(shutdown=True)\n\n","type":"content","url":"/gfm-dask-objectstorage#shutdown-cluster","position":13},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Visualize results"},"type":"lvl2","url":"/gfm-dask-objectstorage#visualize-results","position":14},{"hierarchy":{"lvl1":"Save GFM results in cloud object store","lvl2":"Visualize results"},"content":"To demonstrate that we do not need to download the whole file, we will only plot\nparts of one calculated data variable (mean of ensemble likelihood).\n\n# Lazy-load the ZARR store with xarray\nds = xr.open_zarr(store=zarr_store)\n\n# Plot parts of mean of ensemble likelihood\nplt.figure()\nds.ensemble_likelihood[:5000, :5000].plot(cmap=\"Reds\")\nplt.title(\"Mean of GFM Likelihood Pakistan September 2022\")\nplt.show()","type":"content","url":"/gfm-dask-objectstorage#visualize-results","position":15},{"hierarchy":{"lvl1":"Refine STAC query using filters"},"type":"lvl1","url":"/gfm-filter","position":0},{"hierarchy":{"lvl1":"Refine STAC query using filters"},"content":"You can find the \n\nfilter STAC API extension here.\n\nThe filter extension references behavior defined in the \n\nOGC API - Features - Part 3: Filtering and the Common Query Language (CQL2) and \n\nCommon Query Language (CQL2) specifications.\n\nThe following queryables and combinations thereof are used:\n\nEqui7Tile\n\nratio_after_blob_removal\n\nflooded_pixels\n\nanomaly_detected\n\nsensing_date\n\nMore examples are shown \n\nhere.\n\nfrom pystac_client import Client\n\napi_url = \"https://stac.eodc.eu/api/v1\"\neodc_catalog = Client.open(api_url)\n\n","type":"content","url":"/gfm-filter","position":1},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Filter for Equi7Tile"},"type":"lvl2","url":"/gfm-filter#filter-for-equi7tile","position":2},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Filter for Equi7Tile"},"content":"\n\nfilt = {\n    \"op\": \"eq\",\n    \"args\": [\n        {\"property\": \"Equi7Tile\"}, \"AS020M_E045N012T3\"\n    ]\n}\n\nsearch = eodc_catalog.search(collections=\"GFM\", filter=filt)\n\nprint(\"We found\", search.matched(), \"items, that match our filter criteria.\")\n\n","type":"content","url":"/gfm-filter#filter-for-equi7tile","position":3},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Filter for ratio_after_blob_removal"},"type":"lvl2","url":"/gfm-filter#filter-for-ratio-after-blob-removal","position":4},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Filter for ratio_after_blob_removal"},"content":"\n\nfilt = {\n    \"op\": \"gt\",\n    \"args\": [\n        {\"property\": \"ratio_after_blob_removal\"}, 0.9\n    ]\n}\n\nsearch = eodc_catalog.search(collections=\"GFM\", filter=filt)\n\nprint(\"We found\", search.matched(), \"items, that match our filter criteria.\")\n\n","type":"content","url":"/gfm-filter#filter-for-ratio-after-blob-removal","position":5},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Filter for flooded_pixels"},"type":"lvl2","url":"/gfm-filter#filter-for-flooded-pixels","position":6},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Filter for flooded_pixels"},"content":"\n\nfilt = {\n    \"op\": \"gt\",\n    \"args\": [\n        {\"property\": \"flooded_pixels\"}, 5000000\n    ]\n}\n\nsearch = eodc_catalog.search(collections=\"GFM\", filter=filt)\n\nprint(\"We found\", search.matched(), \"items, that match our filter criteria.\")\n\n","type":"content","url":"/gfm-filter#filter-for-flooded-pixels","position":7},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Filter for anomaly_detected"},"type":"lvl2","url":"/gfm-filter#filter-for-anomaly-detected","position":8},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Filter for anomaly_detected"},"content":"Currently, this property is not available, but will be added soon!\n\nfilt = {\n    \"op\": \"eq\",\n    \"args\": [\n        {\"property\": \"anomaly_detected\"}, True\n    ]\n}\n\nsearch = eodc_catalog.search(collections=\"GFM\", filter=filt)\n\nprint(\"We found\", search.matched(), \"items, that match our filter criteria.\")\n\n","type":"content","url":"/gfm-filter#filter-for-anomaly-detected","position":9},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Combinations of queryables"},"type":"lvl2","url":"/gfm-filter#combinations-of-queryables","position":10},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"Combinations of queryables"},"content":"\n\nfilt = {\n    \"op\": \"and\",\n    \"args\": [\n        {\n            \"op\": \"eq\",\n            \"args\": [\n                {\"property\": \"Equi7Tile\"}, \"AS020M_E045N012T3\"\n            ]\n        },\n        {\n            \"op\": \"gt\",\n            \"args\": [\n                {\"property\": \"ratio_after_blob_removal\"}, 0.7\n            ]\n        },\n    ]\n}\n\nsearch = eodc_catalog.search(collections=\"GFM\", filter=filt)\n\nprint(\"We found\", search.matched(), \"items, that match our filter criteria.\")\n\n","type":"content","url":"/gfm-filter#combinations-of-queryables","position":11},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"List ID of found items"},"type":"lvl2","url":"/gfm-filter#list-id-of-found-items","position":12},{"hierarchy":{"lvl1":"Refine STAC query using filters","lvl2":"List ID of found items"},"content":"\n\nfor item in search.item_collection():\n    print(item.id)","type":"content","url":"/gfm-filter#list-id-of-found-items","position":13},{"hierarchy":{"lvl1":"EODC Dask Tutorial"},"type":"lvl1","url":"/gfm-maximum-flood-extent-dask","position":0},{"hierarchy":{"lvl1":"EODC Dask Tutorial"},"content":"Dask is a flexible parallel computing library for analytics that enables you to\nscale your computations from a single machine to a cluster. This tutorial will\nguide you through the basics of using Dask on the EODC cluster. The computation\nwill take place on the cluster and you only need to download the result to your\nlocal machine for visualisation or further processing steps. As input data\nfor this tutorial we will use output data from the Global Flood Monitoring (GFM)\nservice.\n\nAs an example, we will calculate the maximum flood extent of a certain time range\nover an area of interest in Pakistan.\n\n","type":"content","url":"/gfm-maximum-flood-extent-dask","position":1},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Prerequisites"},"type":"lvl2","url":"/gfm-maximum-flood-extent-dask#prerequisites","position":2},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Prerequisites"},"content":"Before we start, make sure you have installed all the necessary Python libraries and\npackages with the correct versions. It is important that the cluster and client\n(your machine) have the same versions for the key Python libraries. The easiest\nway is to create a new Python environment with the package manager of your liking.\nSee the required dependencies in the \n\nEODC cluster image repository.\n\nIn order to spin up a dedicated cluster on the EODC cluster, you will need to\nrequest an EODC account. Please follow the instructions \n\nhere.\n\n","type":"content","url":"/gfm-maximum-flood-extent-dask#prerequisites","position":3},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"First some imports"},"type":"lvl2","url":"/gfm-maximum-flood-extent-dask#first-some-imports","position":4},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"First some imports"},"content":"\n\nimport pyproj\nimport rioxarray\nimport xarray as xr\nfrom datetime import datetime\nfrom shapely.geometry import box\nfrom pystac_client import Client\nfrom odc import stac as odc_stac\nimport matplotlib.pyplot as plt\n\nfrom eodc.dask import EODCDaskGateway\n\n","type":"content","url":"/gfm-maximum-flood-extent-dask#first-some-imports","position":5},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Initialize cluster"},"type":"lvl2","url":"/gfm-maximum-flood-extent-dask#initialize-cluster","position":6},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Initialize cluster"},"content":"Your username of your EODC account come here, usually it is your email address\nyou have used for registration.\nAfter running the next cell, a prompt will open and ask you to enter your\npassword.\n\nyour_username = \"your.email@address.com\"\ngateway = EODCDaskGateway(username=your_username)\n\nOnce authenticated, you can specify the details of your cluster. Specify the\nnumber of cores and size of memory of your worker machines.\nAdditionally, you can specify any public docker image which has the same dask\nversion installed like our Dask cluster. However, we suggest you are using our\ncluster image to avoid errors coming from mismatching software versions.\n\nYou will find more detailed information about dask\n\n\nhere.\n\nThe client object provides you with an URL to the dashboard of your current\ncluster. Copy/paste this into your Firefox browser to get an overview what is\ncurrently happening on your cluster.\n\nPlease make sure to re-use or shutdown an existing cluster, before spawning a\nnew one!\n\n# Define cluster options\ncluster_options = gateway.cluster_options()\n\n# Set the number of cores per worker\ncluster_options.worker_cores = 8\n\n# Set the memory per worker (in GB)\ncluster_options.worker_memory = 16\n\n# Specify the Docker image to use for the workers\ncluster_options.image = \"ghcr.io/eodcgmbh/cluster_image:2025.7.1\"\n\n# Create a new cluster with the specified options\ncluster = gateway.new_cluster(cluster_options)\n\n# Automatically scale the cluster between 1 and 10 workers based on workload\ncluster.adapt(1, 10)  \n\n# Optionally, scale the cluster to use only one worker\n# cluster.scale(1)\n\n# Get a Dask client for the cluster\nclient = cluster.get_client()\nclient.dashboard_link\n\n","type":"content","url":"/gfm-maximum-flood-extent-dask#initialize-cluster","position":7},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Search and load data"},"type":"lvl2","url":"/gfm-maximum-flood-extent-dask#search-and-load-data","position":8},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Search and load data"},"content":"Now we will define our area (AOI) and time range of interest for which we want\nto calculate the maximum flood extent for.\n\nAll GFM data is registered as a \n\nSTAC collection.\nPlease find more information about STAC in our \n\ndocumentation.\n\n# Define the API URL\napi_url = \"https://stac.eodc.eu/api/v1\"\n\n# Define the STAC collection ID\ncollection_id = \"GFM\"\n\n# Define the area of interest (AOI) as a bounding box\naoi = box(67.398376, 26.197341, 69.027100, 27.591066)\n\n# Define the time range for the search\ntime_range = (datetime(2022, 9, 1), datetime(2022, 10, 1))\n\n# Open the STAC catalog using the specified API URL\neodc_catalog = Client.open(api_url)\n\n# Perform a search in the catalog with the specified parameters\nsearch = eodc_catalog.search(\n    max_items=1000,             # Maximum number of items to return\n    collections=collection_id,  # The collection to search within\n    intersects=aoi,             # The area of interest\n    datetime=time_range         # The time range for the search\n)\n\n# Collect the found items into an item collection\nitems = search.item_collection()\n\nprint(f\"On EODC we found {len(items)} items for the given search query\")\n\nWe will use the found STAC items to (lazy) load the data into a xarray.Dataset\nobject. In order to achieve this, we need to specify the bands which we want to\nload. To calculate the maximum flood extent, we are interested in the\n“ensemble_flood_extent” layer of each GFM item. Furthermore, we need to specify\nthe coordinate reference system (CRS) as well as the resolution of the data. All\nnecessary metadata is saved in each STAC item.\n\n# Extract the coordinate reference system (CRS) from the first item's properties\ncrs = pyproj.CRS.from_wkt(items[0].properties[\"proj:wkt2\"])\n\n# Set the resolution of the data\nresolution = items[0].properties['gsd']\n\n# Specify the bands to load\nbands = [\"ensemble_flood_extent\"]\n\n# Load the data using odc-stac with the specified parameters\nxx = odc_stac.load(\n    items, \n    bbox=aoi.bounds,   # Define the bounding box for the area of interest\n    crs=crs,   # Set the coordinate reference system\n    bands=bands,   # Specify the bands to load\n    resolution=resolution,   # Set the resolution of the data\n    dtype='uint8',   # Define the data type\n    chunks={\"x\": 1000, \"y\": 1000, \"time\": -1},  # Set the chunk size for Dask\n)\n\n# Extract the 'ensemble_flood_extent' data from the loaded dataset\ndata = xx.ensemble_flood_extent\n\n","type":"content","url":"/gfm-maximum-flood-extent-dask#search-and-load-data","position":9},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Process on the cluster"},"type":"lvl2","url":"/gfm-maximum-flood-extent-dask#process-on-the-cluster","position":10},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Process on the cluster"},"content":"First, we filter the data to exclude invalid values and calculate the sum along\nthe time dimension. The maximum flood extent refers to the largest area covered\nby flooded pixels during the specified time range. Therefore, we convert the\nresult to a binary mask where each pixel is set to 1 if it was flooded during\nthe specified time range, and 0 if it was not. Then we start the computation on\nthe cluster and save the result as a compressed TIFF file. This file can be\nvisualized in e.g. QGIS.\n\n# Filter the data to exclude values of 255 (nodata) and 0 (no-flood), then sum\n# along the \"time\" dimension \nresult = data.where((data != 255) & (data != 0)).sum(dim=\"time\")\n\n# Convert the result to binary (1 where the sum is greater than 0, otherwise 0)\n# and set the data type to uint8 \nresult = xr.where(result > 0, 1, 0).astype(\"uint8\")\n\n# Compute the result\ncomputed_result = result.compute(sync=True)\n\n# Save the computed result to a GeoTIFF file with LZW compression\ncomputed_result.rio.to_raster(\"./max_flood_pakistan_202209.tif\", compress=\"LZW\")\n\nAlso, we can plot a part of the result with the plotting library matplotlib.\n\nplt.figure()\ncomputed_result[:5000, :5000].plot()\nplt.title(\"GFM Maximum Flood Extent\")\nplt.show()\n\n","type":"content","url":"/gfm-maximum-flood-extent-dask#process-on-the-cluster","position":11},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Shutdown cluster"},"type":"lvl2","url":"/gfm-maximum-flood-extent-dask#shutdown-cluster","position":12},{"hierarchy":{"lvl1":"EODC Dask Tutorial","lvl2":"Shutdown cluster"},"content":"After successful processing, we need to shutdown our cluster to free up\nresources.\n\n# After a restart of the Jupyter kernel, the cluster object is gone. Use the\n# following command to connect to the cluster again\n# cluster = gateway.connect(gateway.list_clusters()[0].name)\n\n# Shutdown cluster\ncluster.shutdown()","type":"content","url":"/gfm-maximum-flood-extent-dask#shutdown-cluster","position":13},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC"},"type":"lvl1","url":"/gfm-maximum-flood-extent-local","position":0},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC"},"content":"As an example, we will calculate the maximum flood extent of a certain time range\nover an area of interest in Argentina. In March 2025, the area south of Buenos\nAires suffered from severe weather and floods.\n\n","type":"content","url":"/gfm-maximum-flood-extent-local","position":1},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC","lvl2":"First some imports"},"type":"lvl2","url":"/gfm-maximum-flood-extent-local#first-some-imports","position":2},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC","lvl2":"First some imports"},"content":"\n\nimport pyproj\nimport rioxarray # noqa\nimport xarray as xr\nfrom datetime import datetime\nfrom shapely.geometry import box\nfrom pystac_client import Client\nfrom odc import stac as odc_stac\n\n","type":"content","url":"/gfm-maximum-flood-extent-local#first-some-imports","position":3},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC","lvl2":"Search and load data"},"type":"lvl2","url":"/gfm-maximum-flood-extent-local#search-and-load-data","position":4},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC","lvl2":"Search and load data"},"content":"We will define our area (AOI) and time range of interest for which we want\nto calculate the maximum flood extent for. For defining a bounding box, you can\nuse \n\nthis web tool.\n\nAll GFM data is registered as a \n\nSTAC collection.\nPlease find more information about STAC in our \n\ndocumentation.\n\n# Define the API URL\napi_url = \"https://stac.eodc.eu/api/v1\"\n\n# Define the STAC collection ID\ncollection_id = \"GFM\"\n\n# Define the area of interest (AOI) as a bounding box\naoi = box(-62.030182,-37.485756,-60.540161,-36.589068)\n\n# Define the time range for the search\ntime_range = (datetime(2025, 3, 1), datetime(2025, 4, 1))\n\n# Open the STAC catalog using the specified API URL\neodc_catalog = Client.open(api_url)\n\n# Perform a search in the catalog with the specified parameters\nsearch = eodc_catalog.search(\n    max_items=1000,             # Maximum number of items to return\n    collections=collection_id,  # The collection to search within\n    intersects=aoi,             # The area of interest\n    datetime=time_range         # The time range for the search\n)\n\n# Collect the found items into an item collection\nitems = search.item_collection()\n\nprint(f\"On EODC we found {len(items)} items for the given search query\")\n\nWe will use the found STAC items to load the data into a xarray.Dataset\nobject. In order to achieve this, we need to specify the bands which we want to\nload. To calculate the maximum flood extent, we are interested in the\n“ensemble_flood_extent” layer of each GFM item. Furthermore, we need to specify\nthe coordinate reference system (CRS) as well as the resolution of the data. All\nnecessary metadata is saved in each STAC item.\n\n# Extract the coordinate reference system (CRS) from the first item's properties\ncrs = pyproj.CRS.from_wkt(items[0].properties[\"proj:wkt2\"])\n\n# Set the resolution of the data\nresolution = items[0].properties['gsd']\n\n# Specify the bands to load\nbands = [\"ensemble_flood_extent\"]\n\n# Load the data using odc-stac with the specified parameters\nxx = odc_stac.load(\n    items, \n    bbox=aoi.bounds,   # Define the bounding box for the area of interest\n    crs=crs,   # Set the coordinate reference system\n    bands=bands,   # Specify the bands to load\n    resolution=resolution,   # Set the resolution of the data\n    dtype='uint8',   # Define the data type\n    groupby=\"solar_day\",\n    # fail_on_error=False,\n)\n\nxx\n\n","type":"content","url":"/gfm-maximum-flood-extent-local#search-and-load-data","position":5},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC","lvl2":"Process locally"},"type":"lvl2","url":"/gfm-maximum-flood-extent-local#process-locally","position":6},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC","lvl2":"Process locally"},"content":"First, we filter the data to exclude invalid values and calculate the sum along\nthe time dimension. The maximum flood extent refers to the largest area covered\nby flooded pixels during the specified time range. Therefore, we convert the\nresult to a binary mask where each pixel is set to 1 if it was flooded during\nthe specified time range, and 0 if it was not. Then we start the computation on\nthe cluster and save the result as a compressed TIFF file. This file can be\nvisualized in e.g. QGIS.\n\n# Filter the data to exclude values of 255 (nodata) and 0 (no-flood), then sum\n# along the \"time\" dimension \ndata = xx[\"ensemble_flood_extent\"]\nresult = data.where((data != 255) & (data != 0)).sum(dim=\"time\")\n\n# Convert the result to binary (1 where the sum is greater than 0, otherwise 0)\n# and set the data type to uint8 \nresult = xr.where(result > 0, 1, 0).astype(\"uint8\")\n\n# Save the computed result to a GeoTIFF file with LZW compression\nresult.rio.to_raster(\"./max_flood_argentina_202503.tif\", compress=\"LZW\")\n\n","type":"content","url":"/gfm-maximum-flood-extent-local#process-locally","position":7},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC","lvl2":"Plot with matplotlib"},"type":"lvl2","url":"/gfm-maximum-flood-extent-local#plot-with-matplotlib","position":8},{"hierarchy":{"lvl1":"GFM maximum flood extent with STAC","lvl2":"Plot with matplotlib"},"content":"Additionally, we can plot a part of the result with the Python library matplotlib.\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\ncmap = ListedColormap(['none', 'blue'])\n\n\nplt.figure()\nplt.imshow(result[:5000, :5000], cmap=cmap)\nplt.title(\"GFM Maximum Flood Extent\")\nplt.show()","type":"content","url":"/gfm-maximum-flood-extent-local#plot-with-matplotlib","position":9},{"hierarchy":{"lvl1":"Computation of GFM Maximum Flood Extent for a specific area and time of interest"},"type":"lvl1","url":"/gfm-maximum-flood-extent-simple-plot","position":0},{"hierarchy":{"lvl1":"Computation of GFM Maximum Flood Extent for a specific area and time of interest"},"content":"# Necessary python imports\n\nfrom pystac_client import Client\nfrom odc import stac as odc_stac\nimport pyproj\nimport xarray as xr\nfrom shapely.geometry import Polygon\nimport matplotlib.pyplot as plt\n\n# Search against the EODC STAC API\ncatalog = Client.open(\n  \"https://stac.eodc.eu/api/v1\"\n)\n\n# Define your area of interest\naoi = {\n  \"type\": \"Polygon\",\n  \"coordinates\": [\n    [\n      [16.432800292968754, 50.747318126029434],\n      [17.87887573242188, 50.747318126029434],\n      [17.87887573242188, 51.306150195330034],\n      [16.432800292968754, 51.306150195330034],\n      [16.432800292968754, 50.747318126029434]\n    ]\n  ]\n}\n\n# Define your temporal range\ndaterange = {\"interval\": [\"2024-09-18T00:00:00Z\", \"2024-09-25T00:00:00Z\"]}\n\n# Define your search with CQL2 syntax\nsearch = catalog.search(filter_lang=\"cql2-json\", filter={\n  \"op\": \"and\",\n  \"args\": [\n    {\"op\": \"s_intersects\", \"args\": [{\"property\": \"geometry\"}, aoi]},\n    {\"op\": \"anyinteracts\", \"args\": [{\"property\": \"datetime\"}, daterange]},\n    {\"op\": \"=\", \"args\": [{\"property\": \"collection\"}, \"GFM\"]}\n  ]\n})\n\n# Retrieve all found items\nitems = search.item_collection()\nprint(\"We found\", len(items), \"items, that match our filter criteria.\")\n\n# Derive Equi7Grid CRS from first found item\ncrs = pyproj.CRS.from_wkt(items[0].properties[\"proj:wkt2\"])\n\n# Define assets to load\nassets = [\"ensemble_flood_extent\"]\n\n# Get bounding box from AOI\npolygon = Polygon(aoi['coordinates'][0])\n\n# Load asset data into xarray using odc-stac\n# Adjust chunk size of x/y according to available RAM\nxx = odc_stac.load(\n    items, \n    bbox=polygon.bounds,\n    crs=crs,\n    bands=assets,\n    dtype=\"uint8\",\n    chunks={\"x\": 2000, \"y\": 2000, \"time\": -1}, \n    resolution=20)\nxx.ensemble_flood_extent\n\n# Mask data which is nodata (255) and non-flood (0)\nxx = xx.where((xx != 255) & (xx != 0))\n\n# Compute sum along time dimension\ndata = xx.sum(dim=\"time\").astype(\"uint8\")\ndata.ensemble_flood_extent\n\n# Save result as GeoTiff to disk\nimport os\nimport rioxarray\nos.makedirs(\"./maximum_flood_data\", exist_ok=True)\ndata.ensemble_flood_extent.rio.to_raster(\"./maximum_flood_data/result_equi7.tif\", compress=\"LZW\")\n\ndata.ensemble_flood_extent.plot()\nplt.title(\"GFM Maximum Flood Extent\")\nplt.show()","type":"content","url":"/gfm-maximum-flood-extent-simple-plot","position":1},{"hierarchy":{"lvl1":"Compute maximum flood extent utilizing STAC"},"type":"lvl1","url":"/gfm-maximum-flood-extent-stac","position":0},{"hierarchy":{"lvl1":"Compute maximum flood extent utilizing STAC"},"content":"# Some necessary imports\n\nfrom pystac_client import Client\nfrom datetime import datetime\nfrom odc import stac as odc_stac\nimport pyproj\nimport rioxarray # noqa\nimport xarray as xr\nfrom shapely.geometry import box\n\n# Define asset name to use\nasset_name = \"ensemble_flood_extent\"\n\n# Define bounding box\naoi = box(16.77, 49.91, 18.62, 51.25)\n\n# Define time range\ntime_range = (datetime(2024, 9, 18), datetime(2024, 9, 28))\n\n# EODC STAC API URL\napi_url = \"https://stac.eodc.eu/api/v1\"\neodc_catalog = Client.open(api_url)\n\n# Define search query using pystac_client\nsearch = eodc_catalog.search(\n    max_items=1000,\n    collections=\"GFM\",\n    intersects=aoi,\n    datetime=time_range\n)\n\n# Get all found items\nitems = search.item_collection()\nprint(\"We found\", len(items), \"items, that match our filter criteria.\")\n\n# Derive Equi7Grid CRS from first found item\ncrs = pyproj.CRS.from_wkt(items[0].properties[\"proj:wkt2\"])\n\n# Load asset data into xarray using odc-stac\n# Adjust chunk size of x/y according to available RAM\nxx = odc_stac.load(\n    items, \n    bbox=aoi.bounds,\n    crs=crs,\n    bands=[\"ensemble_flood_extent\"],\n    dtype=\"uint8\",\n    chunks={\"x\": 5000, \"y\": 5000, \"time\": -1}, \n    resolution=20)\n\nOptionally, you can save the xarray as a ZARR data store for future use. It’s important to note that the CRS (Coordinate Reference System) needs to be reapplied to the xarray after reading it.\n\n# Optional: Save data as ZARR data store\n# xx.to_zarr(\"./maximum_flood_data/maximum_flood.zarr\")\n\n# Read data from ZARR data store \n# xx = xr.open_zarr(\"./maximum_flood_data/maximum_flood.zarr\")\n# xx.rio.write_crs(crs, inplace=True)\n\n# Mask out data which is nodata (255) and no-flood (0)\nxx = xx.where((xx != 255) & (xx != 0))\n\n# Calculate sum over time dimension\ndata = xx.sum(dim=\"time\").astype(\"uint8\")\n\n# Save result in current CRS (Equi7Grid)\ndata.ensemble_flood_extent.rio.to_raster(\"./maximum_flood_data/result_equi7.tif\", compress=\"LZW\")\n\n# Optional: Reproject data to WebMercator (EPSG:3857)\ndata = data.rio.reproject(\"EPSG:3857\")\ndata.ensemble_flood_extent.rio.to_raster(\"./maximum_flood_data/result_epsg3857.tif\", compress=\"LZW\")","type":"content","url":"/gfm-maximum-flood-extent-stac","position":1},{"hierarchy":{"lvl1":"Plot GFM flood scene"},"type":"lvl1","url":"/gfm-plot-flood-scene","position":0},{"hierarchy":{"lvl1":"Plot GFM flood scene"},"content":"from pystac_client import Client\nfrom datetime import datetime\nfrom odc import stac as odc_stac\nimport pyproj\nimport rioxarray # noqa\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport contextily as cx\n\n# Method to derive the sensing date from a Sentinel-1 scene identifier\ndef get_sensing_date(scene:str) -> datetime:\n    parts = scene.split(\"_\")\n    return datetime.strptime(parts[4], \"%Y%m%dT%H%M%S\")\n\n# Define Sentinel-1 scene identifier and asset name to plot\nscene_id = \"S1A_IW_GRDH_1SDV_20220930T224602_20220930T224627_045240_056863\"\nasset_name = \"ensemble_flood_extent\"\n\napi_url = \"https://stac.eodc.eu/api/v1\"\neodc_catalog = Client.open(api_url)\n\nsearch = eodc_catalog.search(\n    collections=[\"GFM\"],\n    datetime=get_sensing_date(scene_id),\n)\n\nprint(\"We found\", search.matched(), \"items, that match our filter criteria.\")\n\n# Get found STAC items\nitems = search.item_collection()\n\n# Retrieve the CRS from the first found STAC item\ncrs = pyproj.CRS.from_wkt(items[0].properties[\"proj:wkt2\"])\n\n# Load data of selected asset\ndata = odc_stac.load(\n    items, \n    crs=crs,\n    bands=[asset_name],\n    resolution=20,\n    dtype='uint8',\n    )\n\n# Get rid of single time axis\ndata = data[asset_name].squeeze() \n\n\n# Find min/max of valid flood data\nmask = data == 1\nindices = np.argwhere(mask.values)\nminx, maxx = np.min(indices[:, 0]), np.max(indices[:, 0])\nminy, maxy = np.min(indices[:, 1]), np.max(indices[:, 1])\n\ndata = data[minx:maxx, miny:maxy]\n\n# Reproject data to \"WebMercator\" to be able to plot on a basemap\nreproj_data = data.rio.reproject(\"EPSG:3857\")\nreproj_data = reproj_data.where((reproj_data == 1)).astype(\"float16\")\n\n\n# Reduce spatial extent due to memory constraints\nplot_data = reproj_data[0:5000,0:5000]\n\n\n# Plot data and add basemap\nax = plot_data.plot(figsize=(5, 5), label=\"Flood\", add_colorbar=False)\n\ncx.add_basemap(ax.axes, crs='EPSG:3857', source=cx.providers.OpenStreetMap.Mapnik)\n\nplt.title(f\"{scene_id}\\n{asset_name}\")\n\nplt.show()","type":"content","url":"/gfm-plot-flood-scene","position":1},{"hierarchy":{"lvl1":"Linux Basics"},"type":"lvl1","url":"/linux","position":0},{"hierarchy":{"lvl1":"Linux Basics"},"content":"Linux is the basis of a lot of different operating systems. Many companies and developers release their own versions of linux called distributions or distros.\nThe biggest companies being RedHat (RedHat Enterprise Linux) and Canonical (Ubuntu).","type":"content","url":"/linux","position":1},{"hierarchy":{"lvl1":"Linux Basics","lvl2":"Linux basics cheat sheet"},"type":"lvl2","url":"/linux#linux-basics-cheat-sheet","position":2},{"hierarchy":{"lvl1":"Linux Basics","lvl2":"Linux basics cheat sheet"},"content":"","type":"content","url":"/linux#linux-basics-cheat-sheet","position":3},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Basic commands","lvl2":"Linux basics cheat sheet"},"type":"lvl3","url":"/linux#basic-commands","position":4},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Basic commands","lvl2":"Linux basics cheat sheet"},"content":"Some symbols have special meanings in the terminal:\n\n/ refers to the root directory\n\n. refers to your current working directory\n\n~ refers to the current users home directory\n\nCommand\n\nWhat it does\n\nNavigate the filesystem\n\n\n\nls\n\nLists the files in your current working directory\n\ncd <path or directory>\n\nMove through the file system using either relative or absolute paths. cd .. to go up one directory\n\npwd\n\nPrints your current working directory\n\nFile manipulation\n\n\n\ncp <src> <dest>\n\nCopies files or directories\n\nmv <src> <dest>\n\nMoves files or directories\n\nrm [-r] <file or directory>\n\nDeletes files (add -r to delete recursively)\n\ncat <filename>\n\nPrint out the contents of a file\n\nless <filename>\n\nPrints out the contents of a file but with the ability to scroll and search by typing /. Quit with q\n\nnano <filename>\n\nIs a basic and easy to use text editor\n\nmkdir <directoryname>\n\nCreates a new directory\n\ntouch <filename>\n\nCreates a new empty file\n\nMiscellaneous\n\n\n\nsudo <command>\n\nGives the user admin privileges for the command\n\nman <command>\n\nOpens a manual for the given command\n\nhistory\n\nPrints previously entered commands","type":"content","url":"/linux#basic-commands","position":5},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Managing packages","lvl2":"Linux basics cheat sheet"},"type":"lvl3","url":"/linux#managing-packages","position":6},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Managing packages","lvl2":"Linux basics cheat sheet"},"content":"Different distros use different tools to manage packages called package managers.\\n\nI will use nano as an example package:\\n\nNano is a popular and very easy to use terminal text editor.\\n\nOther examples would be Vim or Emacs but those aren’t as beginner friendly\\n\nDebian and distros based on it  use apt:sudo apt update && sudo apt upgrade     # updates the system\napt search nano                         # search for nano in repositories\nsudo apt install nano                   # installs nano\nsudo apt remove nano                    # to remove the package\n\nRedHat based distros like Almalinux use dnf:sudo dnf update         # updates the system\ndnf search nano         # search for nano in the repositories\nsudo dnf install nano   # installs nano\nsudo dnf remove nano    # to remove the package","type":"content","url":"/linux#managing-packages","position":7},{"hierarchy":{"lvl1":"Linux Basics","lvl2":"Users"},"type":"lvl2","url":"/linux#users","position":8},{"hierarchy":{"lvl1":"Linux Basics","lvl2":"Users"},"content":"","type":"content","url":"/linux#users","position":9},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Creating a new user","lvl2":"Users"},"type":"lvl3","url":"/linux#creating-a-new-user","position":10},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Creating a new user","lvl2":"Users"},"content":"The command used to create a new user is called useradd.\n\nJust by running useradd your_user a new user named your_user will be created, but you probably want to add some parameters.\nThe options of the useradd command are:\n\n-m This will create a home directory in /home/your_user\n\n-G group_name will add the newly created user to the here specified groups (wheel is an often used group since it allows the user to use sudo)\n\n-s path/to/shell changes the default shell of the user. The available shells and their paths can be checked with cat /etc/shells. If the shell you want to use is not available it can be installed using the OSs package manager.","type":"content","url":"/linux#creating-a-new-user","position":11},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Modifying an existing user","lvl2":"Users"},"type":"lvl3","url":"/linux#modifying-an-existing-user","position":12},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Modifying an existing user","lvl2":"Users"},"content":"In case you forgot to set something when creating the user, you can do that afterwards using the usermod command.\n\n-aG to add another group to a user\n\n-s to change the users default shell\n\nFor any additional options you might want to know you can always check the man pages using man usermod or any other command.","type":"content","url":"/linux#modifying-an-existing-user","position":13},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"How to add an SSH key to a user?","lvl2":"Users"},"type":"lvl3","url":"/linux#how-to-add-an-ssh-key-to-a-user","position":14},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"How to add an SSH key to a user?","lvl2":"Users"},"content":"This guide shows how to add an SSH key to an existing user with a home directory.\nThe SSH daemon queries the .ssh/authorized_keys file and checks the content of this file against the provided private key.\n\nIf you are the responsible person of a project, and you don’t have access to an EODC VM please send your public key to \n\nsupport@eodc.eu and we will add the key for you.\n\nYou have to use the file with the .pub extension. The other key without a file extension is your private key and should never be shared with anybody. The private key is easily recognized by its beginning:\n-----BEGIN OPENSSH PRIVATE KEY-----[remote_user@eodc ~]$ echo \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCmqcvQgmx6mPuhOsp...oC0oy1oiCRTcCNU4TWKwdpWEzxw== your_email@example.com\" >> ~/.ssh/authorized_keys  \n[remote_user@eodc ~]$ cat .ssh/authorized_keys    \n    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCmqcvQgmx6mPuhOsp...oC0oy1oiCRTcCNU4TWKwdpWEzxw== your_email@example.com\n[remote_user@eodc ~]$ chmod 0600 ~/.ssh/authorized_keys","type":"content","url":"/linux#how-to-add-an-ssh-key-to-a-user","position":15},{"hierarchy":{"lvl1":"Linux Basics","lvl2":"Groups"},"type":"lvl2","url":"/linux#groups","position":16},{"hierarchy":{"lvl1":"Linux Basics","lvl2":"Groups"},"content":"You can check the manual of the groupadd command in the terminal with man groupadd.\n\nThis article goes hand in hand with the User Creation Article.\n\nTypically, a dedicated group is required for read and write access to your private EO-Storage. We will provide you a group ID in our welcome e-mail. After creating a VM, you will need to create the group separtely on each VM. Afterwards all users that shall have access to your private EO-Storage must be added to the group.","type":"content","url":"/linux#groups","position":17},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Create a group","lvl2":"Groups"},"type":"lvl3","url":"/linux#create-a-group","position":18},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Create a group","lvl2":"Groups"},"content":"To create a group, use the groupadd command:\ngroupadd -g GROUP_ID group_name\n\nExample using group ID 5000:\ngroupadd -g 5000 group_name","type":"content","url":"/linux#create-a-group","position":19},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Add a user to the group","lvl2":"Groups"},"type":"lvl3","url":"/linux#add-a-user-to-the-group","position":20},{"hierarchy":{"lvl1":"Linux Basics","lvl3":"Add a user to the group","lvl2":"Groups"},"content":"Often used flags are -a (append) and -G (group).\nThis tells usermod we are appending to the group name that follows the option.ubuntu@eodc:~$ sudo usermod -aG sudo your_user\nubuntu@eodc:~$ sudo usermod -aG group2,group3,group4 your_user\n\nYou can check the user groups with the id commandubuntu@eodc:~$ id your_user","type":"content","url":"/linux#add-a-user-to-the-group","position":21},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask"},"type":"lvl1","url":"/openeo-processes","position":0},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask"},"content":"","type":"content","url":"/openeo-processes","position":1},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"Overview"},"type":"lvl2","url":"/openeo-processes#overview","position":2},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"Overview"},"content":"EODC’s openEO backend can be extended with community processes. This documentation explains the steps you need to follow if you want to add your own process. There is also an \n\nofficial guide in the \n\nopenEO-processes-dask github.","type":"content","url":"/openeo-processes#overview","position":3},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"1. Example Process"},"type":"lvl2","url":"/openeo-processes#id-1-example-process","position":4},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"1. Example Process"},"content":"The process by which the procedure is described is a process to detect deep moist convection. Deep moist convection (DC) is associated with dangerous weather phenomena such as torrential rain, flash floods, large hail and tornadoes. The release of latent heat inside deep convective clouds often plays a crucial role. Studies have shown that DC and overshooting cloud tops penetrate into the lowest stratosphere and enable the exchange of gases from the troposphere deep into the stratosphere. The Sentinel satellites offer the possibility to monitor DC around the globe, independent of the emissivity of the ground. Detecting Deep Moist Convection (DDMC) is a combination of Deep Moist Convection, low- and mid-level cloudiness.\nThe template for the process comes from Stavros Dafis and can be found on \n\nsentinelhub.","type":"content","url":"/openeo-processes#id-1-example-process","position":5},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"2. Preparation"},"type":"lvl2","url":"/openeo-processes#id-2-preparation","position":6},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"2. Preparation"},"content":"To add your own process to openeo, three different files must be prepared. One is the .py file with the process, one .py file with tests and the other is a .json file with all the information about the process.","type":"content","url":"/openeo-processes#id-2-preparation","position":7},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl3":"Python process file","lvl2":"2. Preparation"},"type":"lvl3","url":"/openeo-processes#python-process-file","position":8},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl3":"Python process file","lvl2":"2. Preparation"},"content":"With the Python file for the process, it is important that it really only contains the relevant code and no tests or other implementations. The process should also be written in one function if possible. Apart from the function, the file should contain all the required (openeo) libraries and small labels or explanations of the procedure.\nYou find examples for python scripts in the \n\nopenEO-processes-dask github.\n\nTo add the process to the list of processes, you need to add your process to the openeo_processes_dask/process_implementations/__init__.py file.\n\nAll example files for DDMC can be found at the bottom of this document.","type":"content","url":"/openeo-processes#python-process-file","position":9},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl3":"Python test file","lvl2":"2. Preparation"},"type":"lvl3","url":"/openeo-processes#python-test-file","position":10},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl3":"Python test file","lvl2":"2. Preparation"},"content":"The process must be sufficiently tested before it can be integrated into the production system. A test file must therefore be created. All possible errors and problems must be tested here using abstract data. Possible problems are incorrect input or coding errors, for example.","type":"content","url":"/openeo-processes#python-test-file","position":11},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl3":"JSON file","lvl2":"2. Preparation"},"type":"lvl3","url":"/openeo-processes#json-file","position":12},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl3":"JSON file","lvl2":"2. Preparation"},"content":"So that others can find and use the new process, it must be described in a JSON file. Information about the use case and the input and output of the process is recorded here. Examples can also be found in a \n\ngithub repository.","type":"content","url":"/openeo-processes#json-file","position":13},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"3. Pushing the process into the openEO processes dask"},"type":"lvl2","url":"/openeo-processes#id-3-pushing-the-process-into-the-openeo-processes-dask","position":14},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"3. Pushing the process into the openEO processes dask"},"content":"First you need to create a fork of both GitHub repositories, \n\nopeneo​-processes​-dask and \n\nopeneo-processes.\n\nThe repositories can then be cloned to the local system.\ngit clone https://github.com/<YOUR USER NAME>/openeo-processes-dask.git\n\nWith poetry install --all-extras, all versions of libraries currently used on openEO can be downloaded.\nIf you want to add a new dependency run: poetry add some_new_dependency\n\nAll changes must be committed and pushed into a new branch of your openeo-processes-dask fork. Then a pull request can be created to merge the fork into the openeo/openeo-processes-dask.\nHowever, the following points should be noted beforehand:\n\nAdd comments and documentation for your code\n\nMake sure your tests still run through and add additional tests\n\nFormat your code nicely automatically after every commit - run poetry run pre-commit install and pre-commit run --all-files once, then it’ll work for every commit\n\nAdd a descriptive comment to your commit and push your code to your openeo-processes-dask fork\n\nCreate a PR with a descriptive title for your changes\n\nTo add the .json file to eodcgmbh/openeo-processes, you need to push your file into a branch in your fork and create a pull request.\n\nAfter you requested a pull request, you need to wait for an approval from one of the repository-maintainers. They either approve your pull request or request changes.","type":"content","url":"/openeo-processes#id-3-pushing-the-process-into-the-openeo-processes-dask","position":15},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"4. Example code"},"type":"lvl2","url":"/openeo-processes#id-4-example-code","position":16},{"hierarchy":{"lvl1":"Adding a new process to the openEO processing dask","lvl2":"4. Example code"},"content":"The .py process file for ddmc looks like this:from openeo_processes_dask.process_implementations.arrays import array_element\nfrom openeo_processes_dask.process_implementations.cubes.general import add_dimension\nfrom openeo_processes_dask.process_implementations.cubes.merge import merge_cubes\nfrom openeo_processes_dask.process_implementations.cubes.reduce import reduce_dimension\nfrom openeo_processes_dask.process_implementations.data_model import RasterCube\n\n__all__ = [\"ddmc\"]\n\n\ndef ddmc(\n    data: RasterCube,\n    nir08=\"nir08\",\n    nir09=\"nir09\",\n    cirrus=\"cirrus\",\n    swir16=\"swir16\",\n    swir22=\"swir22\",\n    gain=2.5,\n    target_band=None,\n):\n    dimension = data.openeo.band_dims[0]\n    if target_band is None:\n        target_band = dimension\n\n    # Mid-Level Clouds\n    def MIDCL(data):\n        # B08 = array_element(data, label=nir08, axis = axis)\n\n        B08 = data.sel(**{dimension: nir08})\n\n        # B09 = array_element(data, label=nir09, axis = axis)\n\n        B09 = data.sel(**{dimension: nir09})\n\n        MIDCL = B08 - B09\n\n        MIDCL_result = MIDCL * gain\n\n        return MIDCL_result\n\n    # Deep moist convection\n    def DC(data):\n        # B10 = array_element(data, label=cirrus, axis = axis)\n        # B12 = array_element(data, label=swir22, axis = axis)\n\n        B10 = data.sel(**{dimension: cirrus})\n        B12 = data.sel(**{dimension: swir22})\n\n        DC = B10 - B12\n\n        DC_result = DC * gain\n\n        return DC_result\n\n    # low-level cloudiness\n    def LOWCL(data):\n        # B10 = array_element(data, label=cirrus, axis = axis)\n        # B11 = array_element(data, label=swir16, axis = axis)\n        B10 = data.sel(**{dimension: cirrus})\n        B11 = data.sel(**{dimension: swir16})\n\n        LOWCL = B11 - B10\n\n        LOWCL_result = LOWCL * gain\n\n        return LOWCL_result\n\n    # midcl = reduce_dimension(data, reducer=MIDCL, dimension=dimension)\n    midcl = MIDCL(data)\n    midcl = add_dimension(midcl, name=target_band, label=\"midcl\", type=dimension)\n\n    # dc = reduce_dimension(data, reducer=DC, dimension=dimension)\n    dc = DC(data)\n    # dc = add_dimension(dc, target_band, \"dc\")\n    dc = add_dimension(dc, target_band, label=\"dc\", type=dimension)\n\n    # lowcl = reduce_dimension(data, reducer=LOWCL, dimension=dimension)\n    lowcl = LOWCL(data)\n    lowcl = add_dimension(lowcl, target_band, label=\"lowcl\", type=dimension)\n\n    # ddmc = merge_cubes(merge_cubes(midcl, dc), lowcl)\n    ddmc1 = merge_cubes(midcl, lowcl)\n    ddmc1.openeo.add_dim_type(name=target_band, type=dimension)\n    ddmc = merge_cubes(dc, ddmc1, overlap_resolver=target_band)\n\n    # return a datacube\n    return ddmc\n\n\nThe .py test file for ddmc looks like this:from functools import partial\n\nimport numpy as np\nimport pytest\nimport xarray as xr\nfrom openeo_pg_parser_networkx.pg_schema import BoundingBox, ParameterReference, TemporalInterval\n\nfrom openeo_processes_dask.process_implementations.cubes.load import load_stac\nfrom openeo_processes_dask.process_implementations.cubes.reduce import (\n    reduce_dimension,\n    reduce_spatial,\n)\nfrom openeo_processes_dask.process_implementations.ddmc import ddmc\nfrom openeo_processes_dask.process_implementations.exceptions import ArrayElementNotAvailable\nfrom tests.general_checks import general_output_checks\nfrom tests.mockdata import create_fake_rastercube\n\n\n@pytest.mark.parametrize(\"size\", [(30, 30, 20, 5)])\n@pytest.mark.parametrize(\"dtype\", [np.float32])\ndef test_ddmc_instance_dims(temporal_interval: TemporalInterval, bounding_box: BoundingBox, random_raster_data):\n    input_cube = create_fake_rastercube(\n        data=random_raster_data,\n        spatial_extent=bounding_box,\n        temporal_extent=temporal_interval,\n        bands=[\"nir08\", \"nir09\", \"cirrus\", \"swir16\", \"swir22\"],\n        backend=\"dask\",\n    )\n\n    data = ddmc(input_cube)\n\n    assert isinstance(data, xr.DataArray)\n    assert set(input_cube.dims) == set(data.dims)\n\n@pytest.mark.parametrize(\"size\", [(30, 30, 20, 5)])\n@pytest.mark.parametrize(\"dtype\", [np.float32])\ndef test_ddmc_target_band(temporal_interval: TemporalInterval, bounding_box: BoundingBox, random_raster_data):\n    input_cube = create_fake_rastercube(\n        data=random_raster_data,\n        spatial_extent=bounding_box,\n        temporal_extent=temporal_interval,\n        bands=[\"nir08\", \"nir09\", \"cirrus\", \"swir16\", \"swir22\"],\n        backend=\"dask\",\n    )\n\n    data_band = ddmc(data=input_cube, target_band=\"ddmc\")\n    assert \"ddmc\" in data_band.dims\n\n@pytest.mark.parametrize(\"size\", [(30, 30, 20, 5)])\n@pytest.mark.parametrize(\"dtype\", [np.float32])\ndef test_ddmc_input_cube_exception(temporal_interval: TemporalInterval, bounding_box: BoundingBox, random_raster_data):\n    input_cube_exception = create_fake_rastercube(\n        data=random_raster_data,\n        spatial_extent=bounding_box,\n        temporal_extent=temporal_interval,\n        bands=[\"b04\", \"nir09\", \"cirrus\", \"swir16\", \"swir22\"],\n        backend=\"dask\",\n    )\n\n    with pytest.raises(KeyError):\n        data = ddmc(input_cube_exception)\n\n\nThe .json description file for ddmc looks like this:{\n    \"id\": \"ddmc\",\n    \"summary\": \"Detecting Deep Moist Convection\",\n    \"description\": \"Deep moist convection (DC) is associated with dangerous weather phenomena such as torrential rain, flash floods, large hail and tornadoes. The release of latent heat inside deep convective clouds often plays a crucial role. Studies have shown that DC and overshooting cloud tops penetrate into the lowest stratosphere and enable the exchange of gases from the troposphere deep into the stratosphere. The Sentinel satellites offer the possibility to monitor DC around the globe, independent of the emissivity of the ground. Detecting Deep Moist Convection is a combination of Deep Moist Convection, low- and mid-level cloudiness. \\nThe `data` parameter expects a raster data cube with a dimension, by the default of the type `band` or otherwise another `target_band ` must be specified. By default, the dimension must have at least five bands with the common names `nir08`, `nir09`, `cirrus`, `swir16` and `swir22` assigned. Otherwise, the user has to specify the parameters. The common names for each band are specified in the collection's band metadata and are *not* equal to the band names.\\n\\nBy default, the dimension of type `bands` is renamed by this process. To keep the dimension, specify a new band name in the parameter `target_band`. This adds a new dimension label with the specified name to the dimension, which can be used to access the computed values.\",\n    \"categories\": [\n        \"cubes\",\n        \"math > indices\",\n        \"disaster management and prevention algorithms\"\n    ],\n    \"parameters\": [\n        {\n            \"name\": \"data\",\n            \"description\": \"A raster data cube with five bands that have the common names `nir08`, `nir09`, `cirrus`,  `swir16` and `swir22` assigned.\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"subtype\": \"datacube\",\n                \"dimensions\": [\n                    {\n                        \"type\": \"spatial\",\n                        \"axis\": [\n                            \"x\",\n                            \"y\"\n                        ]\n                    },\n                    {\n                        \"type\": \"bands\"\n                    }\n                ]\n            }\n        },\n        {\n            \"name\": \"nir08\",\n            \"description\": \"The name of the NIR band. Defaults to the band that has the common name `nir` assigned.\\n\\nEither the unique band name (metadata field `name` in bands) or one of the common band names (metadata field `common_name` in bands) can be specified. If the unique band name and the common name conflict, the unique band name has a higher priority.\",\n            \"schema\": {\n                \"type\": \"string\",\n                \"subtype\": \"band-name\"\n            },\n            \"default\": \"nir08\",\n            \"optional\": true\n        },\n\t{\n            \"name\": \"nir09\",\n            \"description\": \"The name of the Water vapour band. Defaults to the band that has the common name `nir09` assigned.\\n\\nEither the unique band name (metadata field `name` in bands) or one of the common band names (metadata field `common_name` in bands) can be specified. If the unique band name and the common name conflict, the unique band name has a higher priority.\",\n            \"schema\": {\n                \"type\": \"string\",\n                \"subtype\": \"band-name\"\n            },\n            \"default\": \"nir09\",\n            \"optional\": true\n        },\n        {\n            \"name\": \"cirrus\",\n            \"description\": \"The name of the SWIR – Cirrus band. Defaults to the band that has the common name `cirrus` assigned.\\n\\nEither the unique band name (metadata field `name` in bands) or one of the common band names (metadata field `common_name` in bands) can be specified. If the unique band name and the common name conflict, the unique band name has a higher priority.\",\n            \"schema\": {\n                \"type\": \"string\",\n                \"subtype\": \"band-name\"\n            },\n            \"default\": \"cirrus\",\n            \"optional\": true\n        },\n\t{\n            \"name\": \"swir16\",\n            \"description\": \"The name of the SWIR (ca 1600 nm) band. Defaults to the band that has the common name `swir16` assigned.\\n\\nEither the unique band name (metadata field `name` in bands) or one of the common band names (metadata field `common_name` in bands) can be specified. If the unique band name and the common name conflict, the unique band name has a higher priority.\",\n            \"schema\": {\n                \"type\": \"string\",\n                \"subtype\": \"band-name\"\n            },\n            \"default\": \"swir16\",\n            \"optional\": true\n        },\n\t{\n            \"name\": \"swir22\",\n            \"description\": \"The name of the SWIR (ca 2200 nm) band. Defaults to the band that has the common name `swir22` assigned.\\n\\nEither the unique band name (metadata field `name` in bands) or one of the common band names (metadata field `common_name` in bands) can be specified. If the unique band name and the common name conflict, the unique band name has a higher priority.\",\n            \"schema\": {\n                \"type\": \"string\",\n                \"subtype\": \"band-name\"\n            },\n            \"default\": \"swir22\",\n            \"optional\": true\n        },\n        {\n            \"name\": \"target_band\",\n            \"description\": \"By default, the dimension is the band dimension. You can specify a new dimension name in this parameter so that a new dimension label with the specified name will be added for the computed values.\",\n            \"schema\": [\n                {\n                    \"type\": \"string\",\n                    \"pattern\": \"^\\\\w+$\"\n                },\n                {\n                    \"type\": \"null\"\n                }\n            ],\n            \"default\": \"band\",\n            \"optional\": true\n        },\n\t{\n            \"name\": \"gain\",\n            \"description\": \"The value by which the indices are to be multiplied. By default, gain is 2.5.\",\n            \"schema\": [\n                {\n                    \"type\": \"double\",\n                    \"pattern\": \"^\\\\w+$\"\n                },\n                {\n                    \"type\": \"null\"\n                }\n            ],\n            \"default\": 2.5,\n            \"optional\": true\n        }\n    ],\n    \"returns\": {\n        \"description\": \"A raster data cube containing the computed DDMC values. The structure of the data cube differs depending on the value passed to `target_band`:\\n `target_band` is a string: The data cube keeps the same dimensions. The dimension properties remain unchanged, but the number of dimension labels for the dimension of type `bands` increases by one. The additional label is named as specified in `target_band`.\",\n        \"schema\": {\n            \"type\": \"object\",\n            \"subtype\": \"datacube\",\n            \"dimensions\": [\n                {\n                    \"type\": \"spatial\",\n                    \"axis\": [\n                        \"x\",\n                        \"y\"\n                    ]\n                }\n            ]\n        }\n    },\n    \"links\": [\n        {\n            \"rel\": \"about\",\n            \"href\": \"https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/deep_moist_convection/\",\n            \"title\": \"DDMC explained by sentinelhub\"\n        },\n        {\n            \"rel\": \"about\",\n            \"href\": \"https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring_vegetation_2.php\",\n            \"title\": \"NDVI explained by NASA\"\n        }\n    ]\n}","type":"content","url":"/openeo-processes#id-4-example-code","position":17},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:"},"type":"lvl1","url":"/distributions","position":0},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:"},"content":"","type":"content","url":"/distributions","position":1},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:"},"type":"lvl1","url":"/distributions#distributions-available-in-our-openstack-cloud-eodc-eu","position":2},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:"},"content":"","type":"content","url":"/distributions#distributions-available-in-our-openstack-cloud-eodc-eu","position":3},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl3":"RedHat Enterprise Linux (RHEL) based:"},"type":"lvl3","url":"/distributions#redhat-enterprise-linux-rhel-based","position":4},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl3":"RedHat Enterprise Linux (RHEL) based:"},"content":"Rocky Linux 8","type":"content","url":"/distributions#redhat-enterprise-linux-rhel-based","position":5},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl3":"Debian based:"},"type":"lvl3","url":"/distributions#debian-based","position":6},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl3":"Debian based:"},"content":"Debian 11\n\nUbuntu22.04LTS","type":"content","url":"/distributions#debian-based","position":7},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl2":"Distributions available in our openstack launcher.eodc.eu:"},"type":"lvl2","url":"/distributions#distributions-available-in-our-openstack-launcher-eodc-eu","position":8},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl2":"Distributions available in our openstack launcher.eodc.eu:"},"content":"","type":"content","url":"/distributions#distributions-available-in-our-openstack-launcher-eodc-eu","position":9},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl3":"RedHat Enterprise Linux (RHEL) based:","lvl2":"Distributions available in our openstack launcher.eodc.eu:"},"type":"lvl3","url":"/distributions#redhat-enterprise-linux-rhel-based-1","position":10},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl3":"RedHat Enterprise Linux (RHEL) based:","lvl2":"Distributions available in our openstack launcher.eodc.eu:"},"content":"Almalinux8\n\nCentOS7\n\nCentOS stream","type":"content","url":"/distributions#redhat-enterprise-linux-rhel-based-1","position":11},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl3":"Debian based:","lvl2":"Distributions available in our openstack launcher.eodc.eu:"},"type":"lvl3","url":"/distributions#debian-based-1","position":12},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl3":"Debian based:","lvl2":"Distributions available in our openstack launcher.eodc.eu:"},"content":"Debian 10\n\nUbuntu16.04 LTS\n\nUbuntu18.04 LTS\n\nUbuntu20.04 LTS","type":"content","url":"/distributions#debian-based-1","position":13},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl2":"Usecases"},"type":"lvl2","url":"/distributions#usecases","position":14},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl2":"Usecases"},"content":"In reality there is no good or bad distribution. Which distro one uses is mostly personal preference and familiarity.\nThe most notable difference would be the package manager which is\n\napt for Debian based distros\n\ndnf for RHEL based distros\n\nAnother difference is the availability of packages through said package managers.\nDebian has a lot of packages in it’s main repositories while RHEL is rather limited though it can alway be expanded by adding new repositories like epel release.\n\nFurther the differences between Debian based distros like Debian and Ubuntu are even less. It mostly comes down to preinstalled packages.\nOr like in the case of Ubuntu the proprietary package manager snap which is used in addition to apt.\nThe same applies to RHEL based distros like Almalinux or CentOS.","type":"content","url":"/distributions#usecases","position":15},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl2":"Getting started"},"type":"lvl2","url":"/distributions#getting-started","position":16},{"hierarchy":{"lvl1":"Distributions available in our openstack cloud.eodc.eu:","lvl2":"Getting started"},"content":"How to launch an instance can be found in our \n\nold knowledgebase. How to add a \n\nSSH key and connect \n\nvia SSH is also covered there.\n\nImportant: \nOn \n\ncloud.eodc.eu the default user is  eodc. \nOn \n\nlauncher.eodc.eu it is the image name of the used distribution i.e. ubuntu or centos.","type":"content","url":"/distributions#getting-started","position":17},{"hierarchy":{"lvl1":"General introduction"},"type":"lvl1","url":"/general","position":0},{"hierarchy":{"lvl1":"General introduction"},"content":"","type":"content","url":"/general","position":1},{"hierarchy":{"lvl1":"General introduction"},"type":"lvl1","url":"/general#general-introduction","position":2},{"hierarchy":{"lvl1":"General introduction"},"content":"","type":"content","url":"/general#general-introduction","position":3},{"hierarchy":{"lvl1":"General introduction","lvl2":"Images"},"type":"lvl2","url":"/general#images","position":4},{"hierarchy":{"lvl1":"General introduction","lvl2":"Images"},"content":"Images for your virtual machine are essentially the operating system. Currently, our available images are:\n\nUbuntu\n\nRocky\n\nDebian\n\nIf you would like to work with another image, please send a request to \n\nsupport@eodc.eu.","type":"content","url":"/general#images","position":5},{"hierarchy":{"lvl1":"General introduction","lvl2":"Flavors"},"type":"lvl2","url":"/general#flavors","position":6},{"hierarchy":{"lvl1":"General introduction","lvl2":"Flavors"},"content":"Flavors are the resources that you can allocate to your virtual machine.\nTheir naming convention consists of the number of CPUs and RAM.","type":"content","url":"/general#flavors","position":7},{"hierarchy":{"lvl1":"General introduction","lvl2":"Volumes"},"type":"lvl2","url":"/general#volumes","position":8},{"hierarchy":{"lvl1":"General introduction","lvl2":"Volumes"},"content":"Volumes are where and how you store your data.\n\nWe offer six different volume types in the EODC Cloud.This is divided between two options for replication and three different performance profiles.\n\nThe figures below are based on a 10GB volume size:\n\nspeed\n\ndesired bw\n\nmax bw\n\ndesired iops\n\nmax iops\n\nslow\n\n2GBps\n\n5GBps\n\n5000\n\n10k\n\nmedium\n\n5GBps\n\n10GBps\n\n15k\n\n30k\n\nultra\n\n20GBps\n\n50GBps\n\n100k\n\n1M\n\nDepending on your specific use case, there are different options to consider. Find some suggestions below:\n\nWe recommend using the default volume size (15GB) for boot devices.\n\nFor additional storage needs, we recommend using additional volumes. This affords much greater flexibility, including allowing for later dynamic size changes.\n\nFor critical data, a replication factor of 3 is recommended.\n\nOur default volume type is “med-3repl”.We feel this offers the best balance between cost and performance while ensuring the highest level of data integrity.We’re happy to change the default volume type in your openstack project, just reach out to \n\nsupport@eodc.eu.","type":"content","url":"/general#volumes","position":9},{"hierarchy":{"lvl1":"Openstack"},"type":"lvl1","url":"/openstack","position":0},{"hierarchy":{"lvl1":"Openstack"},"content":"","type":"content","url":"/openstack","position":1},{"hierarchy":{"lvl1":"Openstack"},"type":"lvl1","url":"/openstack#openstack","position":2},{"hierarchy":{"lvl1":"Openstack"},"content":"We are happy to announce that our new OpenStack environment \n\nEODC Cloud has launched!\n\nThere are certain things that need to be clarified and explained before you can go ahead and start using your resources on our new infrastructure. OpenStack is an open-source cloud operating system consisting of various components which handle virtualised compute resources. We use an OpenStack environment for the cloud infrastructure service to provide resources to our users.\n\nIn order to work with your cloud infrastructure, we will set up an OpenStack “tenant” with a unique identiifer for your company or your project.\nIn OpenStack, a tenant is a logical group of users and resources that are isolated from other tenants. Each tenant has its own set of users, who can launch virtual machines, manage networks and storage resources.\nPlease contact us first for an individual offer via \n\noffice@eodc.eu. After finding a suitable package for you and once your tenant is setup, please create an EODC account following \n\nthis link. Upon confirmation the EODC OpenStack \n\nlauncher will give you full control over your resources.\n\nAccessing your tenant is either possible via the \n\nOpenStack Dashboard or via the \n\nOpenStack API. \nAs a preview, you can see how the Dashboard looks like below:\n\nEODC Knowledgebase & Notebook Gallery\n\nOverview\n\nServices\n\nTutorials\n\nGallery","type":"content","url":"/openstack#openstack","position":3},{"hierarchy":{"lvl1":"Getting Started with OpenStack"},"type":"lvl1","url":"/openstack-starting","position":0},{"hierarchy":{"lvl1":"Getting Started with OpenStack"},"content":"Signing In\n\nGenerate keypair\n\nCreate VM\n\nAssociate floating IP\n\nStart and stop a VM\n\nSuspend to stop\n\nLog into VM\n\nUpload/Share data\n\nAccess /eodc\n\nCreate group and add user","type":"content","url":"/openstack-starting","position":1},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Signing In"},"type":"lvl2","url":"/openstack-starting#signing-in","position":2},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Signing In"},"content":"When you first reach \n\nour Cloud you will see a few options to authenticate with:\n\nEODC Account (Default)\n\nCloud Account\n\nEGI Check-in","type":"content","url":"/openstack-starting#signing-in","position":3},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl4":"EODC Account","lvl2":"Signing In"},"type":"lvl4","url":"/openstack-starting#eodc-account","position":4},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl4":"EODC Account","lvl2":"Signing In"},"content":"To log in to the EODC cloud, each guest must create an EODC account. You can register at \n\neodc Cloud -> Authenticate using EODC Account -> New User? Register\n\nThese are per-user individual accounts, giving greater flexibility over permissions.\n\nThis identity may be federated with one of our supported providers, Microsoft and EGI.\n\nOnce your account is in-place and working, you can make use of these providers after selecting “EODC Account”.\n\nYour identity from these providers will be automatically federated with your EODC Account details.\n\nIf you would like to use the Openstack CLI with your EODC Account this is fully supported.\n\nYou can visit the \n\nApplication Credentials page to manage this.","type":"content","url":"/openstack-starting#eodc-account","position":5},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl4":"Cloud Account","lvl2":"Signing In"},"type":"lvl4","url":"/openstack-starting#cloud-account","position":6},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl4":"Cloud Account","lvl2":"Signing In"},"content":"If you have requested and been granted a Cloud Account, you will be aware of it.\n\nThis is a more traditional username and password approach.","type":"content","url":"/openstack-starting#cloud-account","position":7},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl4":"EGI Check-in","lvl2":"Signing In"},"type":"lvl4","url":"/openstack-starting#egi-check-in","position":8},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl4":"EGI Check-in","lvl2":"Signing In"},"content":"If you are using the EODC Cloud via an EOSC related project then EGI Check-in may be available to you.\n\nThis is can be used exclusively for these use cases.\n\nIf you have a typical project, instead select “EODC Account” and then follow through to EGI.","type":"content","url":"/openstack-starting#egi-check-in","position":9},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Preparation"},"type":"lvl2","url":"/openstack-starting#preparation","position":10},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Preparation"},"content":"","type":"content","url":"/openstack-starting#preparation","position":11},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"For Windows Users","lvl2":"Preparation"},"type":"lvl3","url":"/openstack-starting#for-windows-users","position":12},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"For Windows Users","lvl2":"Preparation"},"content":"","type":"content","url":"/openstack-starting#for-windows-users","position":13},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl4":"Install MobaXTerm","lvl3":"For Windows Users","lvl2":"Preparation"},"type":"lvl4","url":"/openstack-starting#install-mobaxterm","position":14},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl4":"Install MobaXTerm","lvl3":"For Windows Users","lvl2":"Preparation"},"content":"Go to \n\ntheir homepage and download the .exe for the ‘Home Edition’ and install it.\nThis will be your terminal where you can\n\nnavagate the filesystem\n\ncopy data to and from your VM using a graphical file explorer\n\naccess the shell using ssh","type":"content","url":"/openstack-starting#install-mobaxterm","position":15},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"For Linux and Mac Users","lvl2":"Preparation"},"type":"lvl3","url":"/openstack-starting#for-linux-and-mac-users","position":16},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"For Linux and Mac Users","lvl2":"Preparation"},"content":"You can use your Terminal app to connect to your VM","type":"content","url":"/openstack-starting#for-linux-and-mac-users","position":17},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Generate keypair"},"type":"lvl2","url":"/openstack-starting#generate-keypair","position":18},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Generate keypair"},"content":"When logged into Openstack navigate to Compute > Key Pairs in the sidebar.\n\nClick ‘Create Key Pair’\n\n\nType in a name for the keypair you will recognise later (e.g. your name or initials)\n\nFor Key Type use ‘SSH Key’\n\n\nClick ‘Create Key Pair’\n\nAs soon as you create the keypair it will download your private key for you.\nYou need the private key to authenticate when connecting to the VM.\n\nLinux and Mac users should put the downloaded private key into their ~/.ssh/ directory.\nIt should be called id_rsa without a file extension.\nFor Windows the name and location do not matter.\nThe private key is only for you and should never be shared with anyone\nIf someone else needs access to the VM as well they need their own keypair","type":"content","url":"/openstack-starting#generate-keypair","position":19},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Create a VM"},"type":"lvl2","url":"/openstack-starting#create-a-vm","position":20},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Create a VM"},"content":"Navigate to Compute > Instances\n\nClick ‘Launch Instance’\n\nDetails\n\nFill out the name for the VM\n\n\nSource\n\nSelect the image you want to run as your VM\n\nWhen in doubt just select Ubuntu 20.04 LTS\n\n\nFlavour\n\nSelect the resources for your VM (i.e. CPU and RAM)\n\n\nKey Pair\n\nSelect the keypair you created in the previous step\n\n\nClick ‘Launch Instance’\n\nAfter a short while the Status of the VM should change to ‘Active’","type":"content","url":"/openstack-starting#create-a-vm","position":21},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Associate floating IP"},"type":"lvl2","url":"/openstack-starting#associate-floating-ip","position":22},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Associate floating IP"},"content":"Navigate to Compute > Instances\n\nThere you choose the VM you want to have the floating IP\n\nClick the arrow on the right-hand side to open the dropdown menu\n\nClick ‘Associate Floating IP’\n\n\nUnder IP Address select the floating IP\n\nClick ‘Associate’","type":"content","url":"/openstack-starting#associate-floating-ip","position":23},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Log into a VM"},"type":"lvl2","url":"/openstack-starting#log-into-a-vm","position":24},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Log into a VM"},"content":"Connecting to your VM will be done using SSH","type":"content","url":"/openstack-starting#log-into-a-vm","position":25},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"For Windows Users","lvl2":"Log into a VM"},"type":"lvl3","url":"/openstack-starting#for-windows-users-1","position":26},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"For Windows Users","lvl2":"Log into a VM"},"content":"Insert MobaXterm instructions","type":"content","url":"/openstack-starting#for-windows-users-1","position":27},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"For Linux and Mac Users","lvl2":"Log into a VM"},"type":"lvl3","url":"/openstack-starting#for-linux-and-mac-users-1","position":28},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"For Linux and Mac Users","lvl2":"Log into a VM"},"content":"Open you Terminal App and type:ssh <username>@<ip-address>\n\n# In case your key is not in ~/.ssh/ or is called differently add -i\nssh -i path/to/the/key <username>@<ip-address>","type":"content","url":"/openstack-starting#for-linux-and-mac-users-1","position":29},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Creating groups and users"},"type":"lvl2","url":"/openstack-starting#creating-groups-and-users","position":30},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Creating groups and users"},"content":"To access your private storage you need the group ID (gid) we provided you with in the welcome e-mail\n\nIn this example we will use 5000 as the group ID\nThe group name can be whatever, like the name of your project.","type":"content","url":"/openstack-starting#creating-groups-and-users","position":31},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"Creating the group","lvl2":"Creating groups and users"},"type":"lvl3","url":"/openstack-starting#creating-the-group","position":32},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"Creating the group","lvl2":"Creating groups and users"},"content":"First you need to create the group with the gidsudo groupadd -g 5000 group_name","type":"content","url":"/openstack-starting#creating-the-group","position":33},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"Adding a user to the group","lvl2":"Creating groups and users"},"type":"lvl3","url":"/openstack-starting#adding-a-user-to-the-group","position":34},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"Adding a user to the group","lvl2":"Creating groups and users"},"content":"Next you have to add your user to the groupsudo usermod -aG group_name username\n\nYou can check which groups your user is part of use the id command: id username\n\nNow you have to log out and back in again, and you should have access to your private storage.","type":"content","url":"/openstack-starting#adding-a-user-to-the-group","position":35},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Adding users after VM creation"},"type":"lvl2","url":"/openstack-starting#adding-users-after-vm-creation","position":36},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl2":"Adding users after VM creation"},"content":"For other users to have access to the VM you can create additional users.","type":"content","url":"/openstack-starting#adding-users-after-vm-creation","position":37},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"Creating a new user","lvl2":"Adding users after VM creation"},"type":"lvl3","url":"/openstack-starting#creating-a-new-user","position":38},{"hierarchy":{"lvl1":"Getting Started with OpenStack","lvl3":"Creating a new user","lvl2":"Adding users after VM creation"},"content":"There are some things you can specify in this command\nThe most important flags are:\n\n-G to add the user to groups e.g. the one you created in the last step to access your storage\n\n-m to create a home directory for the user under /home/username/\n\nThe command for a basic user looks like that:sudo useradd -m -G group_name username","type":"content","url":"/openstack-starting#creating-a-new-user","position":39},{"hierarchy":{"lvl1":"EODC PyGeoApi"},"type":"lvl1","url":"/pygeoapi","position":0},{"hierarchy":{"lvl1":"EODC PyGeoApi"},"content":"","type":"content","url":"/pygeoapi","position":1},{"hierarchy":{"lvl1":"EODC PyGeoApi"},"type":"lvl1","url":"/pygeoapi#eodc-pygeoapi","position":2},{"hierarchy":{"lvl1":"EODC PyGeoApi"},"content":"This user documentation provides an overview of using the \n\nPyGeoApi deployment at EODC in order to access public and private collections of vector datasets that are available.","type":"content","url":"/pygeoapi#eodc-pygeoapi","position":3},{"hierarchy":{"lvl1":"EODC PyGeoApi","lvl2":"What is PyGeoApi?"},"type":"lvl2","url":"/pygeoapi#what-is-pygeoapi","position":4},{"hierarchy":{"lvl1":"EODC PyGeoApi","lvl2":"What is PyGeoApi?"},"content":"","type":"content","url":"/pygeoapi#what-is-pygeoapi","position":5},{"hierarchy":{"lvl1":"EODC PyGeoApi","lvl3":"Overview","lvl2":"What is PyGeoApi?"},"type":"lvl3","url":"/pygeoapi#overview","position":6},{"hierarchy":{"lvl1":"EODC PyGeoApi","lvl3":"Overview","lvl2":"What is PyGeoApi?"},"content":"PyGeoApi is an open-source Python project that implements OGC (Open Geospatial Consortium) API standards for serving geospatial data over the web. It allows users to publish, access, and interact with geospatial data through modern web-based APIs. pygeoapi supports a variety of geospatial formats and data sources, making it easier to deliver geospatial services and applications for developers and organizations involved in geographic information systems (GIS).","type":"content","url":"/pygeoapi#overview","position":7},{"hierarchy":{"lvl1":"EODC PyGeoApi","lvl3":"Might be appropriate for you?","lvl2":"What is PyGeoApi?"},"type":"lvl3","url":"/pygeoapi#might-be-appropriate-for-you","position":8},{"hierarchy":{"lvl1":"EODC PyGeoApi","lvl3":"Might be appropriate for you?","lvl2":"What is PyGeoApi?"},"content":"Need for Open Standards Compliance: PyGeoApi is built to comply with OGC (Open Geospatial Consortium) standards, ensuring that your geospatial data can be accessed and shared seamlessly with other systems and applications worldwide. By using pygeoapi, you’re assured of delivering data in a way that’s universally compatible and future-proof.\n\nFlexibility in Data Sources and Formats: Whether you’re managing simple or complex geospatial datasets, pygeoapi supports a wide variety of formats like GeoJSON, NetCDF, and GeoTIFF. This flexibility allows you to serve your data from various sources while maintaining high performance and accessibility, ensuring that your customers get the most out of your geospatial assets.\n\nCustomizable, Secure, and Scalable: Our deployment supports both public and private geospatial collections. Private collections can be securely protected behind token-based authentication, giving you control over who can access sensitive data. This ensures that your information is both protected and accessible only to authorized users, while allowing public collections to be shared more broadly. Whether you need to integrate with existing systems or scale up as your operations grow, pygeoapi provides the tools and extensibility to support your evolving requirements.","type":"content","url":"/pygeoapi#might-be-appropriate-for-you","position":9},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO"},"type":"lvl1","url":"/pygeoapi-yipeeo","position":0},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO"},"content":"","type":"content","url":"/pygeoapi-yipeeo","position":1},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Connect to PyGeoApi service"},"type":"lvl2","url":"/pygeoapi-yipeeo#connect-to-pygeoapi-service","position":2},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Connect to PyGeoApi service"},"content":"The YIPEEO vector data is exposed as \n\nOGC API -\nFeatures accessible via\n\n\nEODC’s PyGeoApi.","type":"content","url":"/pygeoapi-yipeeo#connect-to-pygeoapi-service","position":3},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl3":"Available YIPEEO collections","lvl2":"Connect to PyGeoApi service"},"type":"lvl3","url":"/pygeoapi-yipeeo#available-yipeeo-collections","position":4},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl3":"Available YIPEEO collections","lvl2":"Connect to PyGeoApi service"},"content":"Data set ID\n\nDescription\n\nAccess Level\n\npub_yipeeo_yield_fl\n\nYield estimation and prediction at field level.\n\nPublic\n\npub_yipeeo_yield_nuts2\n\nYield estimation and prediction at NUTS2 level.\n\nPublic\n\nprv_yipeeo_yield_field\n\nYield estimation and prediction at field level.\n\nProtected\n\nprv_yipeeo_yield_nuts1\n\nYield estimation and prediction at NUTS1 level.\n\nProtected\n\nprv_yipeeo_yield_nuts2\n\nYield estimation and prediction at NUTS2 level.\n\nProtected\n\nprv_yipeeo_yield_nuts3\n\nYield estimation and prediction at NUTS3 level.\n\nProtected\n\nprv_yipeeo_yield_nuts4\n\nYield estimation and prediction at NUTS4 level.\n\nProtected\n\nprv_yipeeo_fertilize\n\nData on fertilizer use at field level.\n\nProtected\n\nprv_yipeeo_irrigate\n\nData on irrigation use at field level.\n\nProtected\n\nThe protected collections are only accessible using an EODC account.\nThis tutorial demonstates the use of the OGC API - Features making use to the\n\n\nowslib Python client and\nthe \n\nEODC SDK for authentication.import geopandas as gpd\nimport contextily as cx\nfrom rich.console import Console\n\n# EODC SDK\nfrom eodc.auth import DaskOIDC\n\n# OWSLIB\nfrom owslib.ogcapi.features import Features\n\n# EODC OGC API URL\nEODC_OGCAPI_URL = 'https://features.services.eodc.eu/'\n\nconsole = Console()","type":"content","url":"/pygeoapi-yipeeo#available-yipeeo-collections","position":5},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Without Authentication"},"type":"lvl2","url":"/pygeoapi-yipeeo#without-authentication","position":6},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Without Authentication"},"content":"Without Authentication, we can still list all available feature collections, as\nwell as all features from collections which are not protected. But we are not\nable to read features from protected collections.# create eodc_ogcapi object without authentication header\neodc_ogcapi = Features(EODC_OGCAPI_URL)\nfeature_collections = eodc_ogcapi.feature_collections()\nconsole.print(feature_collections)\n\nTry to get items of a protected collection.collection_id = 'prv_yipeeo_yield_field'\n\n# This will fail with an '401 Authorization required' error code\nitems = eodc_ogcapi.collection_items(collection_id)","type":"content","url":"/pygeoapi-yipeeo#without-authentication","position":7},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"With Authentication"},"type":"lvl2","url":"/pygeoapi-yipeeo#with-authentication","position":8},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"With Authentication"},"content":"To read features from protected collections, you need to authenticate with your EODC\ncredentials. Enter your username, typically the email address you used to sign\nup at EODC. A password prompt will be opened automatically.\n\nAfter sucessful authentication, the access token will be used as HTTP header for\nall future requests using OWSLIB.# Enter your username, typically the\n# email address you used to sign up at EODC\nusername = \"firstname.lastname@eodc.eu\" \n\nconn = DaskOIDC(username)\n\nheaders = {\n    \"Authorization\": f\"Bearer {conn.token['access_token']}\"\n}\n\n# add HTTP headers to eodc_ogcapi object\neodc_ogcapi = Features(EODC_OGCAPI_URL, headers=headers)\n\nPrint properties of the first item of the given feature collectionitems = eodc_ogcapi.collection_items(collection_id)\nconsole.print(items['features'][0]['properties'])","type":"content","url":"/pygeoapi-yipeeo#with-authentication","position":9},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Run a query to extract certain features"},"type":"lvl2","url":"/pygeoapi-yipeeo#run-a-query-to-extract-certain-features","position":10},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Run a query to extract certain features"},"content":"Query collection prv_yipeeo_yield_field for\n\ncommon winter wheat (C1111)\n\nwinter barley (C1310), and\n\nfor a given bounding box near Brno\n\nand we only want to have a subset of all attributes (properties)bbox = [16.229703926693578,48.713318232352485,17.472665146572798,49.4680057323523]\n\nselected_props = \"crop_type,crop_id,yield,c_year\"\n\ncql_filter = \"crop_type='common winter wheat' OR crop_type='winter barley'\"\n\n# get all items in the yipeeo_yield_fl collection\nfield_items = eodc_ogcapi.collection_items(\n    collection_id=collection_id,\n    bbox=bbox,\n    limit=2000,\n    properties=selected_props,\n    filter=cql_filter,\n)\nconsole.print(f\"We found {len(field_items['features'])} items matching the query criteria.\")","type":"content","url":"/pygeoapi-yipeeo#run-a-query-to-extract-certain-features","position":11},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Convert features into Geopandas DataFrame"},"type":"lvl2","url":"/pygeoapi-yipeeo#convert-features-into-geopandas-dataframe","position":12},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Convert features into Geopandas DataFrame"},"content":"df = gpd.GeoDataFrame.from_features(field_items[\"features\"], crs=\"EPSG:4326\")\ndf ","type":"content","url":"/pygeoapi-yipeeo#convert-features-into-geopandas-dataframe","position":13},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Plot geometries"},"type":"lvl2","url":"/pygeoapi-yipeeo#plot-geometries","position":14},{"hierarchy":{"lvl1":"PyGeoApi: YIPEEO","lvl2":"Plot geometries"},"content":"ax = df[[\"geometry\"]].plot(\n    facecolor=\"none\", figsize=(12, 6)\n)\ncx.add_basemap(ax, crs=df.crs.to_string(), source=cx.providers.OpenStreetMap.Mapnik)","type":"content","url":"/pygeoapi-yipeeo#plot-geometries","position":15},{"hierarchy":{"lvl1":"HPC"},"type":"lvl1","url":"/hpc","position":0},{"hierarchy":{"lvl1":"HPC"},"content":"","type":"content","url":"/hpc","position":1},{"hierarchy":{"lvl1":"HPC"},"type":"lvl1","url":"/hpc#hpc","position":2},{"hierarchy":{"lvl1":"HPC"},"content":"","type":"content","url":"/hpc#hpc","position":3},{"hierarchy":{"lvl1":"HPC","lvl2":"Why using HPC over the cloud?"},"type":"lvl2","url":"/hpc#why-using-hpc-over-the-cloud","position":4},{"hierarchy":{"lvl1":"HPC","lvl2":"Why using HPC over the cloud?"},"content":"HPC clusters are purpose-built for high-performance tasks. They consist of specialized hardware, high-speed interconnects, and optimized software stacks that makes them significantly faster execution of compute-intensive applications compared to the shared infrastructure of cloud computing, which may be limited in node size and may vary in performance based on the underlying virtualization and resource allocation. Furthermore, while the cloud offers flexibility in scheduling and running workflows, the HPC has a destinct scheduling system that allows to allocate the resources that the user needs for the jobs. This means that it is also more cost-efficient for the user.\nTherefore, for very compute intense tasks which need a large amount of CPUs or GPUs to compute the HPC environment offers a cost-efficient tailored infrastructure.","type":"content","url":"/hpc#why-using-hpc-over-the-cloud","position":5},{"hierarchy":{"lvl1":"HPC","lvl3":"Why using EODC HPC over any other HPC?","lvl2":"Why using HPC over the cloud?"},"type":"lvl3","url":"/hpc#why-using-eodc-hpc-over-any-other-hpc","position":6},{"hierarchy":{"lvl1":"HPC","lvl3":"Why using EODC HPC over any other HPC?","lvl2":"Why using HPC over the cloud?"},"content":"EODC’s large EO data repository is available directly from the HPC. Twelve fast 200Gbps Infiniband links secure that the data is readable with very low latency. The repository mainly holds Sentinel 1, 2 and 3 data but also offers access to a vast amount of other sensors. Additionally EODC offers its customers to download and host data for their projects which means that you may request a collection that you can use on the HPC for processing.\n\nTo find out more about our data offering please visit our \n\nwebsite","type":"content","url":"/hpc#why-using-eodc-hpc-over-any-other-hpc","position":7},{"hierarchy":{"lvl1":"HPC","lvl2":"How to use HPC powered by EODC?"},"type":"lvl2","url":"/hpc#how-to-use-hpc-powered-by-eodc","position":8},{"hierarchy":{"lvl1":"HPC","lvl2":"How to use HPC powered by EODC?"},"content":"To access our HPC system, the project needs to get approved first and the users would need to have appropriate permissions and credentials. This is why we would need you to provide the following information in order for us to get started deploying your project:\n\nWhat is it for?\n\nUse Case Name:\n\nDescription of your project (500 words):\n\nWho is it for?\n\nContact Points for your deployment:\n\nUsers:\n\nNames - Phone Numbers (for 2FA) - SSH public key\n\nFormalities:\n\nHow many CPU/hours will you need:\n\nWhat are your EO Storage requirements connected to VSC?\n\nStart date of the deployment:\n\nDo you need GPU/hours? How many?\n\nOnce we have collected this information from you we will enable access through our cloud infrastructure to the HPC infrastructure.\nIn a welcome e-mail we will provide you with all the neccessary infromation to login to the HPC.","type":"content","url":"/hpc#how-to-use-hpc-powered-by-eodc","position":9},{"hierarchy":{"lvl1":"HPC","lvl3":"Login Guide","lvl2":"How to use HPC powered by EODC?"},"type":"lvl3","url":"/hpc#login-guide","position":10},{"hierarchy":{"lvl1":"HPC","lvl3":"Login Guide","lvl2":"How to use HPC powered by EODC?"},"content":"For Windows users we recommend to use \n\nMobaXterm to setup your connection. Linux users have usually by default an ssh client in the commandline available.\nIf you are an EODC customer who already has access to the cloud infrastructure you will be able to use your VM to login to the HPC. Otherwise we will provide you with a jump host to use to login to the HPC.\n\nAll further needed login information will be provided in our welcome e-mail. Be aware that we need a phone number because a one time password (OTP) will be sent to you via sms on login.","type":"content","url":"/hpc#login-guide","position":11},{"hierarchy":{"lvl1":"HPC","lvl2":"HPC usage"},"type":"lvl2","url":"/hpc#hpc-usage","position":12},{"hierarchy":{"lvl1":"HPC","lvl2":"HPC usage"},"content":"The HPC cluster works with a job scheduler called Slurm. You can submit your jobs in the form of a batch script containing the code you want to run and a header of information needed by the job scheduler.\nSlurm has an excellent \n\ndocumentation available. If you haven’t used it please familiarize yourself before running any jobs at the HPC cluster.","type":"content","url":"/hpc#hpc-usage","position":13},{"hierarchy":{"lvl1":"HPC","lvl3":"Basic commands","lvl2":"HPC usage"},"type":"lvl3","url":"/hpc#basic-commands","position":14},{"hierarchy":{"lvl1":"HPC","lvl3":"Basic commands","lvl2":"HPC usage"},"content":"You can use Slurm as well to display your previous scheduled jobs or to see how busy the cluster is. For this you need a couple of basic commands:\n\nsinfo - will show you an overview of the cluster and the available nodes and their state. See also the \n\nslurm documentation for more detailed view on the potential states that Partition can be in.\n\nsqueue -u [username] replace [username] with your username. The command will show you your current running jobs in the HPC and their status.\n\nsacct  displays accounting information for all jobs\n\nsbatch [scriptname.slrm] - schedule the script [scriptname.slrm] on the cluster. See below for a minimum example.\n\nscancel [jobid] cancels the job with the ID [jobid]. Use squeue -u [username] before to see which jobs you might want to cancel.\n\nA job always creates an output file into your home directory i.e. slurm-[ID].out - which contains the output status messages of your script or your program that you run.","type":"content","url":"/hpc#basic-commands","position":15},{"hierarchy":{"lvl1":"HPC","lvl3":"Minimum example","lvl2":"HPC usage"},"type":"lvl3","url":"/hpc#minimum-example","position":16},{"hierarchy":{"lvl1":"HPC","lvl3":"Minimum example","lvl2":"HPC usage"},"content":"To run an example python script you may want to use the example (called “deftestjob.slrm”) below:#!/bin/bash -e\n#SBATCH --job-name=test_33UXQ\n#SBATCH --time=20:00:00\n#SBATCH -n 128\n\npython calc_ndvi.py 33UXQ\n\n\n\nThere are several \n\nmore options that may be set.\n\nHere is the python script that may be run called calc_ndvi.py. Jobs at this scale in Earth Observation Science are usually split at tile base level. Hence the calc_ndvi.py takes a UTM Tile as input (in this case 33UXQ) and computes the NDVI over all the available files.import os\nimport fnmatch\n\nimport zipfile\nimport numpy as np\nimport rasterio\nfrom rasterio.plot import show\nfrom rasterio.windows import Window\nfrom rasterio.profiles import DefaultGTiffProfile\nimport sys\nimport datetime\nimport shutil\nprint(sys.argv)\n\n\n\n\n\n# Define the root directories to start the search\n\n\nroot_directory = \"/eodc/products/copernicus.eu/s2a_prd_msil1c/\"\nhome_fld = \"/home/fs72030/bschumacher/\"\ndestination_directory = \"~/unpack/\"\noutput_fld = \"/home/fs72030/bschumacher/ndvi_results/\"\n\n\n# Define the search pattern (string to search for)\nsearch_pattern = \"*\"+sys.argv[1]+\"*\"\n\n# Create an empty list to store matching file paths\nmatching_files = []\nfile_names = []\n\n# Walk through the directory tree and find matching files\ncounter = 0\nfor dirpath, dirnames, filenames in os.walk(root_directory):\n    if counter % 100 == 0:\n        print(\"Searching..\" + dirpath)\n    counter += 1\n    for filename in fnmatch.filter(filenames, search_pattern):\n        matching_files.append(os.path.join(dirpath, filename))\n        file_names.append(filename)\n\nfor i in range(0, len(matching_files)):\n#    print(file_names[i])\n#    current_file = file_names[i][:-4]\n\n#current_file = file_names[i][:-4]\n\n# Print the list of matching file paths\n#for file_path in matching_files:\n\n    # Specify the path to the zip file and the destination directory\n    zip_file_path = matching_files[i]\n    \n    current_file = file_names[i][:-4]\n    \n    # Expand the user directory (~) to the full path\n    destination_directory = os.path.expanduser(destination_directory)\n    \n    try:\n        print(datetime.datetime.now())\n        # Check if the destination directory exists, create it if not\n        if not os.path.exists(destination_directory):\n            os.makedirs(destination_directory)\n    \n        # Open and extract the zip file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(destination_directory)\n    \n        print(f\"File '{zip_file_path}' successfully extracted to '{destination_directory}'\")\n    \n    # Define the path to the unzipped .SAFE folder\n    \n        safe_folder = home+\"unpack/\"+current_file+\".SAFE\"\n    \n    # Define the paths to the red and NIR bands\n        granule_lst = os.listdir(os.path.join(safe_folder, \"GRANULE\"))\n        print(granule_lst)\n\n        img_data_lst = os.listdir(os.path.join(safe_folder, \"GRANULE\", granule_lst[0], \"IMG_DATA\"))\n        img_B04 = \"\"\n    \n        for j in range(0,len(img_data_lst)):\n            if \"B04.jp2\" in img_data_lst[j]:\n                img_B04 = img_data_lst[j]\n            if \"B08.jp2\" in img_data_lst[j]:\n                img_B08 = img_data_lst[j]\n\n    #img_data_lst_B08 = fnmatch.filter(os.listdir(os.path.join(safe_folder, \"GRANULE\", granule_lst[0], \"IMG_DATA\")), \"B08.\")\n\n        red_band_path = os.path.join(safe_folder, \"GRANULE\", granule_lst[0], \"IMG_DATA\",img_B04)\n        nir_band_path = os.path.join(safe_folder, \"GRANULE\", granule_lst[0], \"IMG_DATA\",img_B08)\n    \n    # Open the red and NIR bands using rasterio\n        with rasterio.open(red_band_path) as red_band_ds, rasterio.open(nir_band_path) as nir_band_ds:\n        # Read the red and NIR band data as numpy arrays\n            red_band = red_band_ds.read(1)\n            nir_band = nir_band_ds.read(1)\n    \n    \n    \n        # Get metadata from one of the bands (assuming both bands have the same metadata)\n            profile = red_band_ds.profile\n    \n        # Update the profile for the GeoTIFF\n            profile.update(\n                dtype=rasterio.float32,  # Set the data type to float32 for NDVI\n                count=1,  # Set the number of bands to 1 for NDVI\n                driver = 'GTiff'\n            )\n\n\n# Replace 0 values with NaN in the red and NIR bands\n        red_band = np.where(red_band == 0, np.nan, red_band)\n        nir_band = np.where(nir_band == 0, np.nan, nir_band)\n\n    # Compute the NDVI\n        ndvi = (nir_band - red_band) / (nir_band + red_band)\n\n# Define the output GeoTIFF file path\n        output_gtif_path = output_fld+\"ndvi_output_\"+current_file+\".tif\"\n\n\n# Create and write the GeoTIFF file\n        with rasterio.open(output_gtif_path, 'w', **profile) as dst:\n            dst.write(ndvi, 1)  # Write the NDVI data to band 1\n\n        print(f\"NDVI data saved to {output_gtif_path}\")\n        #os.remove(safe_folder)\n        shutil.rmtree(safe_folder)\n        print(\"Deleted \"+safe_folder)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(zip_file_path)\n        pass\n\n\n\n","type":"content","url":"/hpc#minimum-example","position":17},{"hierarchy":{"lvl1":"HPC","lvl2":"Running singularity containers"},"type":"lvl2","url":"/hpc#running-singularity-containers","position":18},{"hierarchy":{"lvl1":"HPC","lvl2":"Running singularity containers"},"content":"Singularity, a container solution born out of necessity for scientific and application-driven workloads, adeptly caters to both established and conventional high-performance computing (HPC) resources. Our HPC system supports running Singularity containers and with native compatibility for InfiniBand and Lustre, it seamlessly integrates with various resource managers such as SLURM and SGE, functioning akin to a regular system command. Notably, Singularity includes inherent backing for MPI and containers requiring GPU resources, further enhancing its versatility for diverse computing needs.\n\nHere we give you a quick overview of the differences between a Virtual Machine, a docker container environment and a singularity container environment:","type":"content","url":"/hpc#running-singularity-containers","position":19},{"hierarchy":{"lvl1":"HPC","lvl3":"Minimum example","lvl2":"Running singularity containers"},"type":"lvl3","url":"/hpc#minimum-example-1","position":20},{"hierarchy":{"lvl1":"HPC","lvl3":"Minimum example","lvl2":"Running singularity containers"},"content":"SBATCH -J myjob\nSBATCH -o output.%j\nSBATCH -p mem_0096\nSBATCH -N 1\n\nspack load singularity %gcc@9.1.0\nsingularity exec docker://python:latest /usr/local/bin/python ./hello.py","type":"content","url":"/hpc#minimum-example-1","position":21},{"hierarchy":{"lvl1":"Object storage @ EODC"},"type":"lvl1","url":"/objectstore","position":0},{"hierarchy":{"lvl1":"Object storage @ EODC"},"content":"","type":"content","url":"/objectstore","position":1},{"hierarchy":{"lvl1":"Object storage @ EODC"},"type":"lvl1","url":"/objectstore#object-storage-eodc","position":2},{"hierarchy":{"lvl1":"Object storage @ EODC"},"content":"","type":"content","url":"/objectstore#object-storage-eodc","position":3},{"hierarchy":{"lvl1":"Object storage @ EODC","lvl2":"Overview"},"type":"lvl2","url":"/objectstore#overview","position":4},{"hierarchy":{"lvl1":"Object storage @ EODC","lvl2":"Overview"},"content":"Object storage is available now at EODC. \nIf you would like to get started with using object storage at EODC, or see a price list, contact \n\nSupport@eodc.eu \n\nTypically, our usage model is a per-project capacity limit, agreed in advance.\nCurrently, the number of buckets, objects within a bucket, and ingress/egress traffic, is subject only to generous fair usage policy.\n\nAvailable interfaces:\n\nSWIFT\n\nS3\n\nEndpoint:\nhttps://objectstore.eodc.eu:2222\n\nIn our Cloud dashboard, be aware that some terminology may be different than you are familiar with.\nFor example, what is commonly referred to as a bucket is known as an container.\nThis is particularly relevant when using the openstack CLI interface.","type":"content","url":"/objectstore#overview","position":5},{"hierarchy":{"lvl1":"Object storage @ EODC","lvl2":"First Use"},"type":"lvl2","url":"/objectstore#first-use","position":6},{"hierarchy":{"lvl1":"Object storage @ EODC","lvl2":"First Use"},"content":"For basic storage usage, the cloud dashboard provides a comfortable way to:\n\nCreate and destroy buckets\n\nMake an existing bucket public\n\nCreate a metadata only object\n\nUpload a file into a bucket\n\nManage folders within a bucket\n\nBe aware all contents in a public bucket are accessible to anyone, anywhere.","type":"content","url":"/objectstore#first-use","position":7},{"hierarchy":{"lvl1":"Object storage @ EODC","lvl2":"Access"},"type":"lvl2","url":"/objectstore#access","position":8},{"hierarchy":{"lvl1":"Object storage @ EODC","lvl2":"Access"},"content":"Access to the object storage requires your EODC Account.\nThis article assumes you already have a working and configured openstack client with access using Application Credentials.","type":"content","url":"/objectstore#access","position":9},{"hierarchy":{"lvl1":"Object storage @ EODC","lvl3":"Generating EC2 Credentials","lvl2":"Access"},"type":"lvl3","url":"/objectstore#generating-ec2-credentials","position":10},{"hierarchy":{"lvl1":"Object storage @ EODC","lvl3":"Generating EC2 Credentials","lvl2":"Access"},"content":"First let’s generate EC2 credentials so we can use the S3 api as well as the SWIFT.\n\nThese credentials will grant full control over all buckets in your project.\nIt is not possible to see any credentials generated by other users within your prject.\n\nBe aware that if you are using object storage across multiple projects, multiple credentials are required.source app-cred-user-openrc.sh\nopenstack ec2 credentials create\n+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field      | Value                                                                                                                                                                |\n+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| access     | 555b59fee5d3490db775dc0721ebed84                                                                                                                                     |\n| links      | {'self': 'https://cloud.eodc.eu:5000/v3/users/1d07b2fabf9921acbeada0d8239fd8ce6f6d50e825eebf3d405903d2db48c017/credentials/OS-EC2/555b59fee5d3490db775dc0721ebed84'} |\n| project_id | XXXXX                                                                                                                                     |\n| secret     | ac49ee2e1ea1421683ecf6c37e9c9f82                                                                                                                                     |\n| trust_id   | None                                                                                                                                                                 |\n| user_id    | 1d07b2fabf9921acbeada0d8239fd8ce6f6d50e825eebf3d405903d2db48c017                                                                                                     |\n+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```bash\n\nNow there is an access key and a secret, these can be used with any S3 friendly tool. e.g. s3cmd, WinSCP, and S3FS.\nNever share your secret with another user.\n\nYou can view these credentials again later.\n```bash\nopenstack ec2 credentials list\n+----------------------------------+----------------------------------+----------------------------------+------------------------------------------------------------------+\n| Access                           | Secret                           | Project ID                       | User ID                                                          |\n+----------------------------------+----------------------------------+----------------------------------+------------------------------------------------------------------+\n| 555b59fee5d3490db775dc0721ebed84 | ac49ee2e1ea1421683ecf6c37e9c9f82 | XXXX                             | 1d07b2fabf9921acbeada0d8239fd8ce6f6d50e825eebf3d405903d2db48c017 |\n+----------------------------------+----------------------------------+----------------------------------+------------------------------------------------------------------+\n\nThese can also be removed later, e.g. if they are compromised.openstack ec credentials delete 555b59fee5d3490db775dc0721ebed84","type":"content","url":"/objectstore#generating-ec2-credentials","position":11}]}
{"version":2,"kind":"Article","sha256":"2838296478acf50b180b79f9d8f5f0f7b6c9b88637821468d8626384541b2209","slug":"hpc","location":"/source/services/hpc.md","dependencies":[],"frontmatter":{"title":"HPC","content_includes_title":true,"numbering":{"title":{"offset":2}},"thumbnail":"/vsc_logo-d0e349987a5c6f8d91e4abffaff6b260.png","exports":[{"format":"md","filename":"hpc.md","url":"/hpc-abc2c8d7fbf3cf628bed73155ddf0b38.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"image","url":"/vsc_logo-d0e349987a5c6f8d91e4abffaff6b260.png","alt":"vsc-logo","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n9KAoAtKHv","urlSource":"../_static/hpc/vsc_logo.png"},{"type":"heading","depth":1,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"HPC","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oWmNk92dB0"}],"identifier":"hpc","label":"HPC","html_id":"hpc","implicit":true,"key":"Ldc0rEw30d"},{"type":"heading","depth":2,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Why using HPC over the cloud?","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"iAXdChKta3"}],"identifier":"why-using-hpc-over-the-cloud","label":"Why using HPC over the cloud?","html_id":"why-using-hpc-over-the-cloud","implicit":true,"key":"Ztz1gbFCLC"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"HPC clusters are purpose-built for high-performance tasks. They consist of specialized hardware, high-speed interconnects, and optimized software stacks that makes them significantly faster execution of compute-intensive applications compared to the shared infrastructure of cloud computing, which may be limited in node size and may vary in performance based on the underlying virtualization and resource allocation. Furthermore, while the cloud offers flexibility in scheduling and running workflows, the HPC has a destinct scheduling system that allows to allocate the resources that the user needs for the jobs. This means that it is also more cost-efficient for the user.\nTherefore, for very compute intense tasks which need a large amount of CPUs or GPUs to compute the HPC environment offers a cost-efficient tailored infrastructure.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"wdDv84bjw7"}],"key":"Zs6N73CdQC"},{"type":"heading","depth":3,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Why using EODC HPC over any other HPC?","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"KDR03D1p48"}],"identifier":"why-using-eodc-hpc-over-any-other-hpc","label":"Why using EODC HPC over any other HPC?","html_id":"why-using-eodc-hpc-over-any-other-hpc","implicit":true,"key":"aFcwokTfEC"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"EODC’s large EO data repository is available directly from the HPC. Twelve fast 200Gbps Infiniband links secure that the data is readable with very low latency. The repository mainly holds Sentinel 1, 2 and 3 data but also offers access to a vast amount of other sensors. Additionally EODC offers its customers to download and host data for their projects which means that you may request a collection that you can use on the HPC for processing.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"rKp12QIqsN"}],"key":"yDsL1XHIUz"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"To find out more about our data offering please visit our ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"WCmrzs5grK"},{"type":"link","url":"https://eodc.eu/","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"website","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"HLPGoFHwgZ"}],"urlSource":"https://eodc.eu/","key":"AXRrRjaiBC"}],"key":"EB05dom9zP"},{"type":"heading","depth":2,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"How to use HPC powered by EODC?","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"p20mHgklPo"}],"identifier":"how-to-use-hpc-powered-by-eodc","label":"How to use HPC powered by EODC?","html_id":"how-to-use-hpc-powered-by-eodc","implicit":true,"key":"YGXmVsMCXU"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"To access our HPC system, the project needs to get approved first and the users would need to have appropriate permissions and credentials. This is why we would need you to provide the following information in order for us to get started deploying your project:","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"KSqr0ZTFDu"}],"key":"nUBPFh3640"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":21,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"What is it for?","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"XTBYCA3Tcz"}],"key":"dk702XNP9S"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use Case Name:","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"kGYVyKvBxW"}],"key":"uRxO0u3Wab"}],"key":"EmFLp9nxWz"},{"type":"listItem","spread":true,"position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Description of your project (500 words):","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"fs5CKfmyKB"}],"key":"Zpzu3TWZ8l"}],"key":"Y3ESUeGjZT"}],"key":"CygeZKA6Rr"}],"key":"AYpMx6b3Ix"},{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Who is it for?","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"NUkM3u3DMF"}],"key":"bjqnZojYYs"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":25,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Contact Points for your deployment:","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"M8KG8hkBgM"}],"key":"mbkxQ9OzM3"}],"key":"WDpb7sqisR"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Users:","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"ZaG1YL5QAQ"}],"key":"k91d2rgYB6"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Names - Phone Numbers (for 2FA) - SSH public key","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"CSdY9q5RN9"}],"key":"sQERoWMsOT"}],"key":"w2AH1CBVIs"}],"key":"U9jnfUxrrH"}],"key":"IY0tJYKAfV"}],"key":"ERwc6UgJlv"}],"key":"K2uW0MaAgK"},{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Formalities:","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"QRcn0TM29P"}],"key":"xQge8kpLUj"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":29,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"How many CPU/hours will you need:","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"yNxls81rGc"}],"key":"oNBoIddLGs"}],"key":"YZpLuJq1BL"},{"type":"listItem","spread":true,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"What are your EO Storage requirements connected to VSC?","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"usJBfrmjq5"}],"key":"tjpptkGyz3"}],"key":"bq6oFwPzpD"},{"type":"listItem","spread":true,"position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Start date of the deployment:","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"dsqbL8P1Iy"}],"key":"B6x4R71DFf"}],"key":"EiLIBwDKwZ"},{"type":"listItem","spread":true,"position":{"start":{"line":32,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Do you need GPU/hours? How many?","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"YiQoZmEwli"}],"key":"DkqjUQE71x"}],"key":"H8iRYMKg6D"}],"key":"axqYJJcgd8"}],"key":"SsoJBe451Z"}],"key":"VSVolz4Fp7"},{"type":"paragraph","position":{"start":{"line":35,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"Once we have collected this information from you we will enable access through our cloud infrastructure to the HPC infrastructure.\nIn a welcome e-mail we will provide you with all the neccessary infromation to login to the HPC.","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"Rrcz2YN4lM"}],"key":"WIXvij1wCo"},{"type":"heading","depth":3,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"Login Guide","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"bja7XdGIof"}],"identifier":"login-guide","label":"Login Guide","html_id":"login-guide","implicit":true,"key":"IyOOURxhBU"},{"type":"paragraph","position":{"start":{"line":42,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"For Windows users we recommend to use ","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"tEBnT9v6rV"},{"type":"link","url":"https://mobaxterm.mobatek.net/","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"MobaXterm","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"kI4lDe6hRC"}],"urlSource":"https://mobaxterm.mobatek.net/","key":"yap200u1N5"},{"type":"text","value":" to setup your connection. Linux users have usually by default an ssh client in the commandline available.\nIf you are an EODC customer who already has access to the cloud infrastructure you will be able to use your VM to login to the HPC. Otherwise we will provide you with a jump host to use to login to the HPC.","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"sT0hEcO7PO"}],"key":"mkJ27VXUwE"},{"type":"paragraph","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"All further needed login information will be provided in our welcome e-mail. Be aware that we need a phone number because a one time password (OTP) will be sent to you via sms on login.","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"r5COeeCTaD"}],"key":"NUXcRlUQjL"},{"type":"heading","depth":2,"position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"HPC usage","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"AFg38fVg1u"}],"identifier":"hpc-usage","label":"HPC usage","html_id":"hpc-usage","implicit":true,"key":"x9q0RlyeQf"},{"type":"paragraph","position":{"start":{"line":50,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"text","value":"The HPC cluster works with a job scheduler called Slurm. You can submit your jobs in the form of a batch script containing the code you want to run and a header of information needed by the job scheduler.\nSlurm has an excellent ","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"Ce5BhUck9S"},{"type":"link","url":"https://slurm.schedmd.com/quickstart.html","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"children":[{"type":"text","value":"documentation","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"YXD24f27vU"}],"urlSource":"https://slurm.schedmd.com/quickstart.html","key":"maMyXWdmwy"},{"type":"text","value":" available. If you haven’t used it please familiarize yourself before running any jobs at the HPC cluster.","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"uhhkU7HtqG"}],"key":"ZQrF7ARzHd"},{"type":"heading","depth":3,"position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"children":[{"type":"text","value":"Basic commands","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"key":"VSZpEDJuv5"}],"identifier":"basic-commands","label":"Basic commands","html_id":"basic-commands","implicit":true,"key":"PvAouAngun"},{"type":"paragraph","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"children":[{"type":"text","value":"You can use Slurm as well to display your previous scheduled jobs or to see how busy the cluster is. For this you need a couple of basic commands:","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"key":"Z8nMVN8mIE"}],"key":"kutnlNR4nv"},{"type":"paragraph","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"inlineCode","value":"sinfo","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"dajdzV5Oci"},{"type":"text","value":" - will show you an overview of the cluster and the available nodes and their state. See also the ","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"DWEWQG1JGU"},{"type":"link","url":"https://slurm.schedmd.com/sinfo.html","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"text","value":"slurm documentation","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"k19T9Di0Vu"}],"urlSource":"https://slurm.schedmd.com/sinfo.html","key":"zuBzjCLW4c"},{"type":"text","value":" for more detailed view on the potential states that Partition can be in.","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"pg9Vnnfx03"}],"key":"wD45TxeB4Z"},{"type":"paragraph","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"inlineCode","value":"squeue -u [username]","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"S0DqclOX0a"},{"type":"text","value":" replace [username] with your username. The command will show you your current running jobs in the HPC and their status.","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"h02gFnK90j"}],"key":"fdNxJyJ9Jp"},{"type":"paragraph","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"inlineCode","value":"sacct","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"key":"MTatgxzYVd"},{"type":"text","value":"  displays accounting information for all jobs","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"key":"NWjaWTsUd6"}],"key":"JOwphw9Oso"},{"type":"paragraph","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"children":[{"type":"inlineCode","value":"sbatch [scriptname.slrm]","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"i57UFhJNng"},{"type":"text","value":" - schedule the script [scriptname.slrm] on the cluster. See below for a minimum example.","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"hDH573qEx2"}],"key":"Xn86nbFLpR"},{"type":"paragraph","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"inlineCode","value":"scancel [jobid]","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"PZoGzoSLT0"},{"type":"text","value":" cancels the job with the ID [jobid]. Use ","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"LE0UTNz8KJ"},{"type":"inlineCode","value":"squeue -u [username]","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"sEy9ulblVk"},{"type":"text","value":" before to see which jobs you might want to cancel.","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"DWUl5DsypU"}],"key":"K2RPkZa6WV"},{"type":"paragraph","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"children":[{"type":"text","value":"A job always creates an output file into your home directory i.e. slurm-[ID].out - which contains the output status messages of your script or your program that you run.","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"Z5f6Niklt7"}],"key":"OJB5f4hHV8"},{"type":"heading","depth":3,"position":{"start":{"line":74,"column":1},"end":{"line":74,"column":1}},"children":[{"type":"text","value":"Minimum example","position":{"start":{"line":74,"column":1},"end":{"line":74,"column":1}},"key":"mVZhC32C48"}],"identifier":"minimum-example","label":"Minimum example","html_id":"minimum-example","implicit":true,"key":"GjAdthH68f"},{"type":"paragraph","position":{"start":{"line":77,"column":1},"end":{"line":77,"column":1}},"children":[{"type":"text","value":"To run an example python script you may want to use the example (called “deftestjob.slrm”) below:","position":{"start":{"line":77,"column":1},"end":{"line":77,"column":1}},"key":"PYTRh34vNS"}],"key":"ztG7Ii6QQf"},{"type":"code","lang":"","value":"#!/bin/bash -e\n#SBATCH --job-name=test_33UXQ\n#SBATCH --time=20:00:00\n#SBATCH -n 128\n\npython calc_ndvi.py 33UXQ\n\n","position":{"start":{"line":79,"column":1},"end":{"line":88,"column":1}},"key":"pfje8gSb3J"},{"type":"paragraph","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"children":[{"type":"text","value":"There are several ","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"key":"Gwux99LXSO"},{"type":"link","url":"https://slurm.schedmd.com/sbatch.html","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"children":[{"type":"text","value":"more options","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"key":"w6wLORrCur"}],"urlSource":"https://slurm.schedmd.com/sbatch.html","key":"m5si88aCAX"},{"type":"text","value":" that may be set.","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"key":"H5AjcxKds2"}],"key":"LXxmCjkzXo"},{"type":"paragraph","position":{"start":{"line":93,"column":1},"end":{"line":93,"column":1}},"children":[{"type":"text","value":"Here is the python script that may be run called calc_ndvi.py. Jobs at this scale in Earth Observation Science are usually split at tile base level. Hence the calc_ndvi.py takes a UTM Tile as input (in this case 33UXQ) and computes the NDVI over all the available files.","position":{"start":{"line":93,"column":1},"end":{"line":93,"column":1}},"key":"o0QMncCyDm"}],"key":"dQiYcoJ0Yf"},{"type":"code","lang":"","value":"import os\nimport fnmatch\n\nimport zipfile\nimport numpy as np\nimport rasterio\nfrom rasterio.plot import show\nfrom rasterio.windows import Window\nfrom rasterio.profiles import DefaultGTiffProfile\nimport sys\nimport datetime\nimport shutil\nprint(sys.argv)\n\n\n\n\n\n# Define the root directories to start the search\n\n\nroot_directory = \"/eodc/products/copernicus.eu/s2a_prd_msil1c/\"\nhome_fld = \"/home/fs72030/bschumacher/\"\ndestination_directory = \"~/unpack/\"\noutput_fld = \"/home/fs72030/bschumacher/ndvi_results/\"\n\n\n# Define the search pattern (string to search for)\nsearch_pattern = \"*\"+sys.argv[1]+\"*\"\n\n# Create an empty list to store matching file paths\nmatching_files = []\nfile_names = []\n\n# Walk through the directory tree and find matching files\ncounter = 0\nfor dirpath, dirnames, filenames in os.walk(root_directory):\n    if counter % 100 == 0:\n        print(\"Searching..\" + dirpath)\n    counter += 1\n    for filename in fnmatch.filter(filenames, search_pattern):\n        matching_files.append(os.path.join(dirpath, filename))\n        file_names.append(filename)\n\nfor i in range(0, len(matching_files)):\n#    print(file_names[i])\n#    current_file = file_names[i][:-4]\n\n#current_file = file_names[i][:-4]\n\n# Print the list of matching file paths\n#for file_path in matching_files:\n\n    # Specify the path to the zip file and the destination directory\n    zip_file_path = matching_files[i]\n    \n    current_file = file_names[i][:-4]\n    \n    # Expand the user directory (~) to the full path\n    destination_directory = os.path.expanduser(destination_directory)\n    \n    try:\n        print(datetime.datetime.now())\n        # Check if the destination directory exists, create it if not\n        if not os.path.exists(destination_directory):\n            os.makedirs(destination_directory)\n    \n        # Open and extract the zip file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(destination_directory)\n    \n        print(f\"File '{zip_file_path}' successfully extracted to '{destination_directory}'\")\n    \n    # Define the path to the unzipped .SAFE folder\n    \n        safe_folder = home+\"unpack/\"+current_file+\".SAFE\"\n    \n    # Define the paths to the red and NIR bands\n        granule_lst = os.listdir(os.path.join(safe_folder, \"GRANULE\"))\n        print(granule_lst)\n\n        img_data_lst = os.listdir(os.path.join(safe_folder, \"GRANULE\", granule_lst[0], \"IMG_DATA\"))\n        img_B04 = \"\"\n    \n        for j in range(0,len(img_data_lst)):\n            if \"B04.jp2\" in img_data_lst[j]:\n                img_B04 = img_data_lst[j]\n            if \"B08.jp2\" in img_data_lst[j]:\n                img_B08 = img_data_lst[j]\n\n    #img_data_lst_B08 = fnmatch.filter(os.listdir(os.path.join(safe_folder, \"GRANULE\", granule_lst[0], \"IMG_DATA\")), \"B08.\")\n\n        red_band_path = os.path.join(safe_folder, \"GRANULE\", granule_lst[0], \"IMG_DATA\",img_B04)\n        nir_band_path = os.path.join(safe_folder, \"GRANULE\", granule_lst[0], \"IMG_DATA\",img_B08)\n    \n    # Open the red and NIR bands using rasterio\n        with rasterio.open(red_band_path) as red_band_ds, rasterio.open(nir_band_path) as nir_band_ds:\n        # Read the red and NIR band data as numpy arrays\n            red_band = red_band_ds.read(1)\n            nir_band = nir_band_ds.read(1)\n    \n    \n    \n        # Get metadata from one of the bands (assuming both bands have the same metadata)\n            profile = red_band_ds.profile\n    \n        # Update the profile for the GeoTIFF\n            profile.update(\n                dtype=rasterio.float32,  # Set the data type to float32 for NDVI\n                count=1,  # Set the number of bands to 1 for NDVI\n                driver = 'GTiff'\n            )\n\n\n# Replace 0 values with NaN in the red and NIR bands\n        red_band = np.where(red_band == 0, np.nan, red_band)\n        nir_band = np.where(nir_band == 0, np.nan, nir_band)\n\n    # Compute the NDVI\n        ndvi = (nir_band - red_band) / (nir_band + red_band)\n\n# Define the output GeoTIFF file path\n        output_gtif_path = output_fld+\"ndvi_output_\"+current_file+\".tif\"\n\n\n# Create and write the GeoTIFF file\n        with rasterio.open(output_gtif_path, 'w', **profile) as dst:\n            dst.write(ndvi, 1)  # Write the NDVI data to band 1\n\n        print(f\"NDVI data saved to {output_gtif_path}\")\n        #os.remove(safe_folder)\n        shutil.rmtree(safe_folder)\n        print(\"Deleted \"+safe_folder)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(zip_file_path)\n        pass\n\n\n\n","position":{"start":{"line":95,"column":1},"end":{"line":237,"column":1}},"key":"kjvMPzw0iF"},{"type":"heading","depth":2,"position":{"start":{"line":240,"column":1},"end":{"line":240,"column":1}},"children":[{"type":"text","value":"Running singularity containers","position":{"start":{"line":240,"column":1},"end":{"line":240,"column":1}},"key":"uOQgv8ZUpN"}],"identifier":"running-singularity-containers","label":"Running singularity containers","html_id":"running-singularity-containers","implicit":true,"key":"WC9HXVLAfZ"},{"type":"paragraph","position":{"start":{"line":242,"column":1},"end":{"line":242,"column":1}},"children":[{"type":"text","value":"Singularity, a container solution born out of necessity for scientific and application-driven workloads, adeptly caters to both established and conventional high-performance computing (HPC) resources. Our HPC system supports running Singularity containers and with native compatibility for InfiniBand and Lustre, it seamlessly integrates with various resource managers such as SLURM and SGE, functioning akin to a regular system command. Notably, Singularity includes inherent backing for MPI and containers requiring GPU resources, further enhancing its versatility for diverse computing needs.","position":{"start":{"line":242,"column":1},"end":{"line":242,"column":1}},"key":"xUS861avL5"}],"key":"T8ShVNgTvc"},{"type":"paragraph","position":{"start":{"line":244,"column":1},"end":{"line":244,"column":1}},"children":[{"type":"text","value":"Here we give you a quick overview of the differences between a Virtual Machine, a docker container environment and a singularity container environment:","position":{"start":{"line":244,"column":1},"end":{"line":244,"column":1}},"key":"AM0apkDogh"}],"key":"RT4P5Baqby"},{"type":"image","url":"/virtualization-71b3d42c09a9d785f7890978fc90c120.png","alt":"Virtulization Example","position":{"start":{"line":246,"column":1},"end":{"line":246,"column":1}},"key":"Ces5Ps3sWi","urlSource":"../_static/hpc/virtualization.png"},{"type":"heading","depth":3,"position":{"start":{"line":250,"column":1},"end":{"line":250,"column":1}},"children":[{"type":"text","value":"Minimum example","position":{"start":{"line":250,"column":1},"end":{"line":250,"column":1}},"key":"n9yKMSUar8"}],"identifier":"minimum-example","label":"Minimum example","html_id":"minimum-example-1","implicit":true,"key":"R2AyYCmBID"},{"type":"code","lang":"#!/bin/bash","value":"SBATCH -J myjob\nSBATCH -o output.%j\nSBATCH -p mem_0096\nSBATCH -N 1\n\nspack load singularity %gcc@9.1.0\nsingularity exec docker://python:latest /usr/local/bin/python ./hello.py","position":{"start":{"line":252,"column":1},"end":{"line":260,"column":1}},"key":"e74hi5pOU6"}],"key":"gMWT7RNqlW"}],"key":"ufnpEgoQ7m"},"references":{"cite":{"order":[],"data":{}}}}